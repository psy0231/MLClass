{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822565c8",
   "metadata": {},
   "source": [
    "### 추천 시스템 제작(Pinterest), CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcd3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "# import cv2\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential  # class\n",
    "from tensorflow.keras.models import load_model  # model 사용\n",
    "from tensorflow.keras.layers import Dense       # 전결합\n",
    "from tensorflow.keras.layers import Dropout     # 특정 node를 사용안함.\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping   # 학습 자동 중지\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # 우수한 학습 모델 파일 저장\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.utils import to_categorical   # one-hot 엔코딩\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split # 학습셋과 테스트셋의 분리 지원\n",
    "from sklearn.model_selection import StratifiedKFold  # K겹 교차 검증\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "import platform \n",
    "# Windows, Linux, Darwin\n",
    "if (platform.system() == 'Windows'):  \n",
    "    rc('font', family=font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name())\n",
    "    path = '.' # Local\n",
    "else:    \n",
    "    rc('font', family='NanumBarunGothic')  # Ubuntu 18.04 기준 한글 처리\n",
    "    path = '/content/drive/My Drive/kd_ml/dnn/recommend_book' # Colab\n",
    "\n",
    "os.chdir(path) # 기본 경로 설정\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12         # 글자 크기\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 4) # 10:4의 그래프 비율\n",
    "plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# gpu 사용시 런타임에서 필요한 양만큼의 GPU 메모리를 할당후 자동 증가 처리\n",
    "# OS 메모리도 초기화됨.\n",
    "# ---------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "    \n",
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d177f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(220, 50)\n"
     ]
    }
   ],
   "source": [
    "# header가 있을경우 skiprows=1 선언\n",
    "data = np.loadtxt('./train.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가, 손실: 0.0060576219111680984\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0d6b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 49)\n",
      "(220,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터와 class의 분리\n",
    "# 0: 강아지, 1: 카페, 2: 가을, 3: 한우, 4: 꽃\n",
    "X = data[:, 0:49]  # 0 ~ 49\n",
    "print(X.shape)\n",
    "Y = data[:, 49]    # 49 번째 데이터, class의 분리\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f97224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:5]) # 5행 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c5141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(Y[:5]) # 5행의 데이터만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe1e4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1\n",
      " 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3\n",
      " 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5\n",
      " 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0\n",
      " 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2\n",
      " 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y = Y.astype('int') # 정수로 형변환\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d0995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 0: 강아지, 1: 카페, 2: 가을, 3: 한우, 4: 꽃\n",
    "Y_encoded = to_categorical(Y) # 데이터 전처리 기법: one-hot-encoding\n",
    "\n",
    "print(Y_encoded[:5]) \n",
    "# [[1. 0. 0. 0. 0.]: 0 <- 숫자 1이 존재하는 index\n",
    "#  [0. 1. 0. 0. 0.]: 1\n",
    "#  [0. 0. 1. 0. 0.]: 2\n",
    "#  [0. 0. 0. 1. 0.]: 3\n",
    "#  [0. 0. 0. 0. 1.]]: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd52142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0.] -> [1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0.] -> [0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0.] -> [0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0], '->', Y_encoded[0]) \n",
    "# [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
    "# 0.] 이 데이터는 답이 [1. 0. 0. 0. 0.] 임\n",
    "\n",
    "print(X[1], '->', Y_encoded[1]) \n",
    "# [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
    "# 0.] 이 데이터는 답이 [0. 1. 0. 0. 0.]임\n",
    "\n",
    "print(X[2], '->', Y_encoded[2]) \n",
    "# [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
    "# 0.]이 데이터는 답이 [0. 0. 1. 0. 0.]임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8974cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "(40, 7)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 분할을 통한 훈련, 검증, 테스트 데이터의 분리\n",
    "seed = 0\n",
    "# X 분할: x_train_all, x_test\n",
    "# Y 분할: y_train_all, y_test\n",
    "# stratify=Y_encoded: 5개의 그룹을 균등하게 분포시켜 분할함.\n",
    "# test_size=0.1: 10%를 x_test, y_test에 할당\n",
    "# random_state=seed: 무작위로 데이터 추출\n",
    "\n",
    "# 90%: x_train_all, y_train_all\n",
    "# 10%: x_test, y_test\n",
    "# 테스트의 크기가 분류 갯수보다 작으면 에러가 발생함으로 5건 이상이어야함.\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(X, Y_encoded,\n",
    "                                           stratify=Y_encoded,\n",
    "                                           test_size=0.1,\n",
    "                                           random_state=seed)\n",
    "\n",
    "# 나머지 데이터 90%를 분할, 80%: 훈련, 20%: 검증\n",
    "# 80%: x_train, y_train\n",
    "# 20%: x_val, y_val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                           stratify=y_train_all,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=seed)\n",
    "\n",
    "print(y_val)\n",
    "print(y_val.shape)\n",
    "# 강아지, 고양이, 물고기, 자전거, 축제, 등산, 캠핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad033c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "(22, 7)\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "# (10, 5): 10건의 데이터가 입력되어 한건당 5가지에 속할 확률이 출력됨으로 10행 5열이됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a43597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 20)                1000      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 77        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,287\n",
      "Trainable params: 1,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.8012 - accuracy: 0.4899\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82500, saving model to .\\Pinterest.h5\n",
      "158/158 [==============================] - 2s 7ms/step - loss: 1.7870 - accuracy: 0.5000 - val_loss: 1.4727 - val_accuracy: 0.8250\n",
      "Epoch 2/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.1937 - accuracy: 0.8408\n",
      "Epoch 2: val_accuracy improved from 0.82500 to 0.87500, saving model to .\\Pinterest.h5\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.1939 - accuracy: 0.8354 - val_loss: 0.8465 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 0.6487 - accuracy: 0.9801\n",
      "Epoch 3: val_accuracy improved from 0.87500 to 1.00000, saving model to .\\Pinterest.h5\n",
      "158/158 [==============================] - 1s 7ms/step - loss: 0.6312 - accuracy: 0.9810 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 0.2622 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 7ms/step - loss: 0.2596 - accuracy: 1.0000 - val_loss: 0.1414 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 6: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 7: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 11: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 12: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 13: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 14: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.7179e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 9.0029e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 9.0032e-04 - accuracy: 1.0000 - val_loss: 8.1121e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 7.6525e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.6141e-04 - accuracy: 1.0000 - val_loss: 6.9492e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 6.5918e-04 - accuracy: 1.0000\n",
      "Epoch 21: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 6.5587e-04 - accuracy: 1.0000 - val_loss: 6.0018e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 5.6800e-04 - accuracy: 1.0000\n",
      "Epoch 22: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 7ms/step - loss: 5.6736e-04 - accuracy: 1.0000 - val_loss: 5.2200e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 4.9705e-04 - accuracy: 1.0000\n",
      "Epoch 23: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 7ms/step - loss: 4.9429e-04 - accuracy: 1.0000 - val_loss: 4.5508e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 4.3405e-04 - accuracy: 1.0000\n",
      "Epoch 24: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 7ms/step - loss: 4.3249e-04 - accuracy: 1.0000 - val_loss: 3.9895e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 3.8857e-04 - accuracy: 1.0000\n",
      "Epoch 25: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.8002e-04 - accuracy: 1.0000 - val_loss: 3.5164e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 3.3771e-04 - accuracy: 1.0000\n",
      "Epoch 26: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.3529e-04 - accuracy: 1.0000 - val_loss: 3.1134e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 2.9642e-04 - accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.9722e-04 - accuracy: 1.0000 - val_loss: 2.7631e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 2.6514e-04 - accuracy: 1.0000\n",
      "Epoch 28: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.6389e-04 - accuracy: 1.0000 - val_loss: 2.4520e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 2.3802e-04 - accuracy: 1.0000\n",
      "Epoch 29: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.3469e-04 - accuracy: 1.0000 - val_loss: 2.1839e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 2.1324e-04 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.0912e-04 - accuracy: 1.0000 - val_loss: 1.9495e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 1.8773e-04 - accuracy: 1.0000\n",
      "Epoch 31: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.8676e-04 - accuracy: 1.0000 - val_loss: 1.7424e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 1.6577e-04 - accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.6702e-04 - accuracy: 1.0000 - val_loss: 1.5626e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.4884e-04 - accuracy: 1.0000\n",
      "Epoch 33: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.4947e-04 - accuracy: 1.0000 - val_loss: 1.4005e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "147/158 [==========================>...] - ETA: 0s - loss: 1.3337e-04 - accuracy: 1.0000\n",
      "Epoch 34: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.3424e-04 - accuracy: 1.0000 - val_loss: 1.2573e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.2130e-04 - accuracy: 1.0000\n",
      "Epoch 35: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.2043e-04 - accuracy: 1.0000 - val_loss: 1.1277e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.0794e-04 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.0832e-04 - accuracy: 1.0000 - val_loss: 1.0161e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 9.5346e-05 - accuracy: 1.0000\n",
      "Epoch 37: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 9.7494e-05 - accuracy: 1.0000 - val_loss: 9.1618e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 8.7718e-05 - accuracy: 1.0000\n",
      "Epoch 38: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 8.7905e-05 - accuracy: 1.0000 - val_loss: 8.2494e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 8.1323e-05 - accuracy: 1.0000\n",
      "Epoch 39: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.9273e-05 - accuracy: 1.0000 - val_loss: 7.4332e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 7.1594e-05 - accuracy: 1.0000\n",
      "Epoch 40: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.1558e-05 - accuracy: 1.0000 - val_loss: 6.7207e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 6.4248e-05 - accuracy: 1.0000\n",
      "Epoch 41: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 6.4546e-05 - accuracy: 1.0000 - val_loss: 6.0684e-05 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 5.9122e-05 - accuracy: 1.0000\n",
      "Epoch 42: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.8357e-05 - accuracy: 1.0000 - val_loss: 5.4849e-05 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 5.2742e-05 - accuracy: 1.0000\n",
      "Epoch 43: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.2774e-05 - accuracy: 1.0000 - val_loss: 4.9595e-05 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 4.8141e-05 - accuracy: 1.0000\n",
      "Epoch 44: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.7730e-05 - accuracy: 1.0000 - val_loss: 4.4914e-05 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 4.3454e-05 - accuracy: 1.0000\n",
      "Epoch 45: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.3201e-05 - accuracy: 1.0000 - val_loss: 4.0676e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 3.8142e-05 - accuracy: 1.0000\n",
      "Epoch 46: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.9115e-05 - accuracy: 1.0000 - val_loss: 3.6924e-05 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 3.5986e-05 - accuracy: 1.0000\n",
      "Epoch 47: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.5438e-05 - accuracy: 1.0000 - val_loss: 3.3351e-05 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 3.1839e-05 - accuracy: 1.0000\n",
      "Epoch 48: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.2091e-05 - accuracy: 1.0000 - val_loss: 3.0270e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 2.8840e-05 - accuracy: 1.0000\n",
      "Epoch 49: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.9126e-05 - accuracy: 1.0000 - val_loss: 2.7486e-05 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 2.6131e-05 - accuracy: 1.0000\n",
      "Epoch 50: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.6430e-05 - accuracy: 1.0000 - val_loss: 2.4944e-05 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 2.4045e-05 - accuracy: 1.0000\n",
      "Epoch 51: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.3980e-05 - accuracy: 1.0000 - val_loss: 2.2593e-05 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 2.1870e-05 - accuracy: 1.0000\n",
      "Epoch 52: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.1772e-05 - accuracy: 1.0000 - val_loss: 2.0542e-05 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.9986e-05 - accuracy: 1.0000\n",
      "Epoch 53: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.9789e-05 - accuracy: 1.0000 - val_loss: 1.8739e-05 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 1.7895e-05 - accuracy: 1.0000\n",
      "Epoch 54: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.8014e-05 - accuracy: 1.0000 - val_loss: 1.7017e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.6445e-05 - accuracy: 1.0000\n",
      "Epoch 55: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.6365e-05 - accuracy: 1.0000 - val_loss: 1.5482e-05 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.4938e-05 - accuracy: 1.0000\n",
      "Epoch 56: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.4894e-05 - accuracy: 1.0000 - val_loss: 1.4075e-05 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.3427e-05 - accuracy: 1.0000\n",
      "Epoch 57: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.3544e-05 - accuracy: 1.0000 - val_loss: 1.2848e-05 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.2332e-05 - accuracy: 1.0000\n",
      "Epoch 58: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.2345e-05 - accuracy: 1.0000 - val_loss: 1.1668e-05 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 1.1226e-05 - accuracy: 1.0000\n",
      "Epoch 59: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.1255e-05 - accuracy: 1.0000 - val_loss: 1.0669e-05 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.0246e-05 - accuracy: 1.0000\n",
      "Epoch 60: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.0246e-05 - accuracy: 1.0000 - val_loss: 9.6857e-06 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 9.2333e-06 - accuracy: 1.0000\n",
      "Epoch 61: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 9.3496e-06 - accuracy: 1.0000 - val_loss: 8.8632e-06 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "153/158 [============================>.] - ETA: 0s - loss: 8.5417e-06 - accuracy: 1.0000\n",
      "Epoch 62: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 8.5144e-06 - accuracy: 1.0000 - val_loss: 8.0496e-06 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 7.7134e-06 - accuracy: 1.0000\n",
      "Epoch 63: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.7825e-06 - accuracy: 1.0000 - val_loss: 7.3552e-06 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 7.0786e-06 - accuracy: 1.0000\n",
      "Epoch 64: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.0786e-06 - accuracy: 1.0000 - val_loss: 6.7174e-06 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 6.5828e-06 - accuracy: 1.0000\n",
      "Epoch 65: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 6.4584e-06 - accuracy: 1.0000 - val_loss: 6.1244e-06 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.9031e-06 - accuracy: 1.0000\n",
      "Epoch 66: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.9031e-06 - accuracy: 1.0000 - val_loss: 5.5849e-06 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 5.4208e-06 - accuracy: 1.0000\n",
      "Epoch 67: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.3863e-06 - accuracy: 1.0000 - val_loss: 5.1170e-06 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 4.9606e-06 - accuracy: 1.0000\n",
      "Epoch 68: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.9064e-06 - accuracy: 1.0000 - val_loss: 4.6253e-06 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 4.5095e-06 - accuracy: 1.0000\n",
      "Epoch 69: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.4771e-06 - accuracy: 1.0000 - val_loss: 4.2468e-06 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 4.0314e-06 - accuracy: 1.0000\n",
      "Epoch 70: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.0878e-06 - accuracy: 1.0000 - val_loss: 3.8475e-06 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 3.7122e-06 - accuracy: 1.0000\n",
      "Epoch 71: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.7279e-06 - accuracy: 1.0000 - val_loss: 3.5435e-06 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 3.3874e-06 - accuracy: 1.0000\n",
      "Epoch 72: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.4095e-06 - accuracy: 1.0000 - val_loss: 3.2306e-06 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 3.0908e-06 - accuracy: 1.0000\n",
      "Epoch 73: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.0979e-06 - accuracy: 1.0000 - val_loss: 2.9117e-06 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 2.8572e-06 - accuracy: 1.0000\n",
      "Epoch 74: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.8240e-06 - accuracy: 1.0000 - val_loss: 2.6822e-06 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "148/158 [===========================>..] - ETA: 0s - loss: 2.6315e-06 - accuracy: 1.0000\n",
      "Epoch 75: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.6000e-06 - accuracy: 1.0000 - val_loss: 2.4676e-06 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 2.4063e-06 - accuracy: 1.0000\n",
      "Epoch 76: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.3872e-06 - accuracy: 1.0000 - val_loss: 2.2709e-06 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 2.1850e-06 - accuracy: 1.0000\n",
      "Epoch 77: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.1722e-06 - accuracy: 1.0000 - val_loss: 2.0504e-06 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.9762e-06 - accuracy: 1.0000\n",
      "Epoch 78: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.9737e-06 - accuracy: 1.0000 - val_loss: 1.8626e-06 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.8140e-06 - accuracy: 1.0000\n",
      "Epoch 79: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.8010e-06 - accuracy: 1.0000 - val_loss: 1.7166e-06 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 1.6610e-06 - accuracy: 1.0000\n",
      "Epoch 80: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.6606e-06 - accuracy: 1.0000 - val_loss: 1.5855e-06 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 1.5387e-06 - accuracy: 1.0000\n",
      "Epoch 81: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.5429e-06 - accuracy: 1.0000 - val_loss: 1.4633e-06 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.4033e-06 - accuracy: 1.0000\n",
      "Epoch 82: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.3928e-06 - accuracy: 1.0000 - val_loss: 1.3232e-06 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.2794e-06 - accuracy: 1.0000\n",
      "Epoch 83: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.2796e-06 - accuracy: 1.0000 - val_loss: 1.2249e-06 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 1.1552e-06 - accuracy: 1.0000\n",
      "Epoch 84: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.1521e-06 - accuracy: 1.0000 - val_loss: 1.0878e-06 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 1.0561e-06 - accuracy: 1.0000\n",
      "Epoch 85: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 1.0412e-06 - accuracy: 1.0000 - val_loss: 9.9838e-07 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "156/158 [============================>.] - ETA: 0s - loss: 9.5367e-07 - accuracy: 1.0000\n",
      "Epoch 86: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 9.5292e-07 - accuracy: 1.0000 - val_loss: 8.9705e-07 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 8.8879e-07 - accuracy: 1.0000\n",
      "Epoch 87: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 8.8879e-07 - accuracy: 1.0000 - val_loss: 8.5533e-07 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "155/158 [============================>.] - ETA: 0s - loss: 8.1447e-07 - accuracy: 1.0000\n",
      "Epoch 88: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 8.0730e-07 - accuracy: 1.0000 - val_loss: 7.5996e-07 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 7.5579e-07 - accuracy: 1.0000\n",
      "Epoch 89: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 7.4166e-07 - accuracy: 1.0000 - val_loss: 7.0930e-07 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 6.4160e-07 - accuracy: 1.0000\n",
      "Epoch 90: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 6.4433e-07 - accuracy: 1.0000 - val_loss: 6.1691e-07 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "149/158 [===========================>..] - ETA: 0s - loss: 6.0565e-07 - accuracy: 1.0000\n",
      "Epoch 91: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 6.0510e-07 - accuracy: 1.0000 - val_loss: 5.6326e-07 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 5.4593e-07 - accuracy: 1.0000\n",
      "Epoch 92: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.4625e-07 - accuracy: 1.0000 - val_loss: 5.2154e-07 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "150/158 [===========================>..] - ETA: 0s - loss: 5.0227e-07 - accuracy: 1.0000\n",
      "Epoch 93: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 5.0174e-07 - accuracy: 1.0000 - val_loss: 4.9472e-07 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 4.6578e-07 - accuracy: 1.0000\n",
      "Epoch 94: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 4.6401e-07 - accuracy: 1.0000 - val_loss: 4.2617e-07 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 3.9633e-07 - accuracy: 1.0000\n",
      "Epoch 95: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.9988e-07 - accuracy: 1.0000 - val_loss: 3.8445e-07 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 3.6674e-07 - accuracy: 1.0000\n",
      "Epoch 96: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.6744e-07 - accuracy: 1.0000 - val_loss: 3.3677e-07 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 3.2939e-07 - accuracy: 1.0000\n",
      "Epoch 97: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.3122e-07 - accuracy: 1.0000 - val_loss: 3.2485e-07 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "152/158 [===========================>..] - ETA: 0s - loss: 3.1136e-07 - accuracy: 1.0000\n",
      "Epoch 98: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 3.1236e-07 - accuracy: 1.0000 - val_loss: 2.7716e-07 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "151/158 [===========================>..] - ETA: 0s - loss: 2.6605e-07 - accuracy: 1.0000\n",
      "Epoch 99: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.6633e-07 - accuracy: 1.0000 - val_loss: 2.5332e-07 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "154/158 [============================>.] - ETA: 0s - loss: 2.5158e-07 - accuracy: 1.0000\n",
      "Epoch 100: val_accuracy did not improve from 1.00000\n",
      "158/158 [==============================] - 1s 6ms/step - loss: 2.5426e-07 - accuracy: 1.0000 - val_loss: 2.5332e-07 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 네트워크 구성\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "tf.random.set_seed(SEED) # Global seed, 가중치, 편향이 일정하게 변경됨\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "model = Sequential()\n",
    "# input_dim=25: 입력 데이터 x의 갯수\n",
    "model.add(Dense(20, input_dim=49, activation='relu'))# Weight: 500, BIAS: 20\n",
    "model.add(Dense(10, activation='relu')) \n",
    "model.add(Dense(7, activation='softmax')) # 0 ~ 1 사이의 확률 5개 출력,총합: 1\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 오차 감소시마다 h5 형식으로 파일 저장\n",
    "mcp = ModelCheckpoint(filepath='./Pinterest.h5', monitor='val_accuracy',\n",
    "                      verbose=1, save_best_only=True)\n",
    "\n",
    "# 오차가 1회 증가시 자동 종료\n",
    "es = EarlyStopping(monitor='loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 validation_data=(x_val, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size=1, callbacks=[mcp, es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef6c87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4cAAAHGCAYAAADHdv52AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB2e0lEQVR4nO3dd3hUZd7/8c/MpFdIQkJo0kSI1GUFFoNSYgGVItIEFXBZXV1EEd0HiYLoj7igyNqwo6J0XVBkEQUEBBERdJGOgFJCCCW9kEzO749hBiaTQBKSzCR5v65rrmTuc+ac78DZ5/HD9z73MRmGYQgAAAAAUKOZ3V0AAAAAAMD9CIcAAAAAAMIhAAAAAIBwCAAAAAAQ4RAAAAAAIMIhAAAAAECEQwAAAACACIcAAAAAAEle7i7AnfLz87V9+3ZFRUXJbCYnAwAAADVVQUGBkpKS1KFDB3l51cyYVDO/9Xnbt29Xp06d3F0GAAAAAA+xZcsWXXfdde4uwy1qdDiMioqSZLsAoqOj3VzNBWfPbtCePXfL3/8atW//jbvLAQAAAKq9xMREderUyZERaqIaHQ7tU0mjo6PVoEEDN1dzQXBwE50+Lfn5nfOougAAAIDqribfblZzv7kHs1iCJElWa4abKwEAAABQUxAOPRDhEAAAAEBlq9HTSj3Khg1SUpIUGytLmC0cFhRkyzCsMpksbi4OAAAAQHVH59BTPPqoNGiQtG2bo3MoSVZrpvtqAgAAAFBj0DksIcMwZLValZ+fXzEnaNJEOn1aOndOxjlDZnNTSVZlZaXIx8enYs6JYnl5eclischkMrm7FAAAAKBSEA4vwzAMpaSkKDk5WVarteJONHaslJ0thYdLhw8rOPgNSYaOHEmR2Uz30B0sFosiIyMVGhpKSAQAAEC1Rzi8jBMnTiglJUUhISEKCQmRl5dXxQQFi0VKS5Oio6XwcGVm5krKl59fA1ksAeV/PhTLMAzl5+crLS1NiYmJys7O9qjnYAIAAAAVgXB4CVarVampqapTp44iIiIq9mTe3rafZrPk56f8fC8ZRr58fb3l5eVXsedGkYKDg+Xr66tTp04pMjJSFgsLAwEAAKD6YkGaS8jLy5NhGAoMDKz4k9mDR0GBJMlksv/VFFT8uVGswMBAGYahvLw8d5cCAAAAVCjCYQlUyv1m5vN/FY5waAuLhlGB9znisrjXEAAAADUF4dBTFAqH9r8aw6BzCAAAAKDiEQ49hT0cnl8R9cK0UjqHAAAAACoe4dBTFLrnULJPK606ncMvvvhCderU0ZEjR8rleN27d9ejjz5aLscCAAAAcGmEQ0/hcs9hxXcO8/Ly9PHHH+v48ePlcrzw8HC1a9dOvr6+5XI8AAAAAJWHcOgpXKaVVnzn8NixY7rnnnt08uTJcjle165d9c033ygyMrJcjgcAAACg8hAOPYXLtFLPWZCmoMD9NQAAAACoWIRDT1HMoywqalrpyJEj1aRJE0lShw4dZDKZ9O2330qSGjdurFmzZmnChAkKCgrSU089JUlavny5evbsqYiICIWFhenOO+9UYmKi45hLly51evTDt99+K5PJpN9++03Dhg1TSEiIGjZsqOeff16GYZSp7l9++UX9+vVT7dq15efnpw4dOmjBggVO+xw9elTDhw9XZGSkAgMD1aVLF2VnZ0uSUlJS9NBDD6levXry9/dXu3bt9Mcff5SpFgAAAKA68bhwaLVaNWvWLHXp0qXEn0lNTVVYWJj69+9fcYVdxDAMWa2Z5fsysm0vx1i2rNZs5ednXPazZQlaL774ojZs2CBJ+vLLL3Xo0CGnP/MFCxYoNzdX33//vUaNGiVJevPNNzVw4EB9++23+vzzz7V792499NBDlz3Xvffeq27dumnjxo0aO3asnnnmGS1atKjUNW/btk1du3ZVQECAli1bpo0bN+q2227TiBEj9P777zv269Onj3Jzc/XNN9/o22+/VZ8+fRzdz3vvvVc7d+7UsmXLtGnTJo0aNUpWKyvCAgAAAF7uLsAuOztbCxYs0IwZM7Rv3z61bt26xJ998cUXdfbs2QqszllBQZY2bAiquBNsKN3u3bplyGIJLNVnIiIi1KBBA0lSvXr11LhxY6ftWVlZevXVV53GPvvsM/n4+Dje//Of/9TDDz982XMNHjzYESLbtGmjVatWafHixRoyZEipan7iiSfUrVs3zZ8/3zHWsWNH5eTkaNKkSRo5cqTOnDmjHTt26PXXX1fbtm0lSdddd51j/zVr1mj27NmOsQ4dOpSqBgAAAKC68pjO4Q8//KAJEyaof//+Gj9+fIk/d+DAAc2ePVs33nhjBVZX8/Tu3dtlzMfHR4mJifrss8/0r3/9S5999pmysrKUlpZ2yWPdcccdTu/bt29f6sdd5OTkaP369br//vtdtg0fPlwnTpzQ3r17FR4erpYtW2r8+PHavHmzy76xsbGaOnWqvvrqq1KdHwAAAKjuPKZz2LFjRx0/fly+vr6aMmVKiT/3wAMPaMKECdqzZ49SUlIqrL6Lmc0B6tYto3wParVKv/xi+71dO+UbWcrO3i+TyUdBQZfuoprNAeVbi6SoqCin93l5ebrvvvu0dOlSXXfddWrRooXq1Kkj6fIL1oSFhTm9DwoKUm5ubqnqOX36tPLz89WwYUOXbdHR0ZKks2fPymQyac2aNXriiScUGxurTp06afr06YqNjZUkLVq0SE899ZT69eun5s2ba9q0aerbt2+pagEAAACqI4/pHAYHB5f6+XizZs1ScnKyHn/88RLtn5ubq7S0NMcrPT29LKXKZDLJYgks35d3sCwmf8fLyytEFou/LBa/y3724kVgyovZ7HxpvPfee1qzZo0OHjyodevW6Z133tHAgQPL/bzFCQ0Nlclk0rFjx1y2nThxQpIcYTU6Oloff/yxDhw4oCZNmqhnz57as2ePJCkkJESvvfaa/vjjD910003q16+fvvnmm0r7HgAAAICn8phwWFrbtm3Ts88+q/nz58vb27tEn0lISFBoaKjjFRMTU8FVloLJVGjFUvujLCpusRT7n1tJung7duxQmzZtVLduXcfY119/XWG1FRYUFKQuXbro3Xffddk2f/58NW/eXFdffbXTeOPGjfXxxx8rKChIGzdudNoWGRmpl19+We3atdO6desqtHYAAACgKqiS4fD06dO66667NH36dF177bUl/tzEiROVmprqeO3atasCqyyDi8LhhUdZFJT5sQ+XU7duXQUHB+vjjz/Wzp07dfLkyWL3bd++vTZs2KD58+fr119/VUJCglatWlUhdRVn+vTpWrNmjYYNG6YNGzZo27Ztmjx5sv7973/rlVdekSQdOXJEo0eP1tq1a7V371698847yszMdKzEOmDAAK1cuVJ79+7V4sWLtX//fseUUwAAAKAm85h7DksqLy9PAwcO1PXXX68xY8aU6rO+vr5OU1cvt5BKpbOHQ6tVJpPPRRsKJFmK+sQVsVgsev311/XEE09o7ty5Wrt2rSIjI4vc9/7779eePXs0btw45eTk6M4779RLL71U5MI1FSU2NlZr167VM888o969e8swDF133XX66quvHAsShYSEKCkpSXfeeafy8vLUpk0bLVu2zPGPCD4+PrrnnnuUmZmpFi1a6O2339Ytt9xSad8BAAAA8FQmo6LaUldgypQpWrp0qX7++WeXbevWrVP37t0v+fm1a9dedh/J9rD0hg0b6siRI47HOlwsJydHhw4dUpMmTeTn51fC6q/Azp1SdrbUooWM4GBlZPwkSQoMbCezuWRTZ1G+Kv0aAAAAgFtcLhvUBFWuc9ixY0dt377dZfyZZ55Renq6Xn75ZTVv3twNlZUDp2mlJtlm/RacfwEAAABAxaky9xwOGTJEM2fOVFBQkNq3b+/yCgsLU2hoqNq3b6+goAp8QH1FumhaqSTHfYcVuSgNAAAAgLKzWq2aNWuWY42L4hiGoRkzZjhmpHXo0KHS1/C4nCoTDg8dOqSjR4+6u4yK5bRaqXRhxVI6hwAAAIAnyc7O1pw5c9SmTRtNmDBBOTk5l9z/ueee04svvqiZM2dq27Zt6t69u+644w7973//q6SKL88j7zmsLB53z+HBg9KZM1LDhlJUlDIzd6qgIFv+/lfLyyu04s8PF9xzCAAAUDOU9p7Db7/9VgMHDtQDDzyg/Px8rVq1qsg1UyTp7Nmzql+/vj755BMNGDDAMd61a1c1b95cH330UXl9jStS5e45rNYKTSu1r1B6qc7hOes5rT20Vpl5mS7bCgqkXbukjIzyLrTmyM/PV2pqqkJDd8jLi/+5AAAAlLfagUGaOPhmd5dRah07dtTx48fl6+urKVOmXHLfVatWycvLS3379nUaHzRokKZPn16BVZYO/7XrSQpNKzWZ7LN+i7/n8KVNL+mpNU9VcGGQa/YGAABAOfBJu0YTB+9xdxkO6enpTo+8K/w4PLvg4OASH/PXX39VTEyMLBbnx9PFxMToxIkTysjI8Ih1UwiHnsR+sTjC4eU7h98f/V6SdHXY1YoMjHR8fPduKSVFMpkkD7jOqrSCggKZzVXm9lwAAIAqJczSyN0lOImJiXF6P3ny5Mt2Bi8nOTlZ4eHhLuNhYWGSbM9fJxzCWRkWpNmVvEuS9Nbtb6lHkx5KSZFuu01K2SQFBEj/+Y90c9Xr0nsM7jkEAACoWXbt2qX69es73hfVNSyt/Pz8IpsNtsfXXfjpboRDT+LyKItLTyvNzsvWoZRDkqSYOjFKSpJuuUX65RepVi3pyy+lrl0ruGYAAACgGgkODlZISEi5HjMkJET79u1zGU9JSZHJZFLt2rXL9XxlxVw5T1JoWunlFqTZd3qfCowC1farrexTkerWzRYMo6KkdesIhgAAAIAnaNGihfbscb2vcs+ePWrevLnHzFAjHHqSUi5IY59S2iQoRt26mbR/v3TVVdKGDVLbthVdLAAAAICSuPnmm3Xq1CmtWbPGaXzRokXq16+fm6pyRTj0JC7TSi/dOdx9arckaee6GB09KrVqJX33nXT11RVfqmR7tovJZFJKSkq57AcAAABUF0OGDNHMmTMlSU2bNtWIESM0evRorVy5Urt27dKjjz6qvXv3asKECW6u9ALCoSdxmVZqX5Dm0p3D3COt1KGDtH69VILndQIAAACoYIcOHdLRo0cd79966y3dcccdGjFihK677jrt2rVLa9euVVRUlBurdMaCNJ6k2GmlRXcO7eFQyTEaO0mKiKjg+gAAAAC4mDJlisvjLrZs2eL03t/fX6+++qpeffXVSqysdOgcehKXR1kUP600z5qn/Wf2294kx6hZs0qoDwAAAEC1RTj0JKV4lMWBMweUX5Av5QZJaQ3UvHnpTtW3b1/16dPHZXzy5MmOB39+9913uv3221W3bl2FhIQoLi5Oe/fuLd2JipGfn68XXnhBrVq1kq+vryIjIzVy5EglJSU57ffBBx+oTZs28vf3V7169fTuu+86ti1fvlydOnVSYGCg6tSpo6lTp5ZLbQAAAEBNxLTSsjAMKSur/I+bny9lZ9t+z8iQqSBHysqWYcqXlOm06+6j222/nGolf38pOrp0pxoxYoRGjBihs2fPOj1XZf78+RozZowk6f3331dsbKyef/55Wa1WjRs3TkOHDtX27dvL+g2dzr969WpNnz5d1113nQ4dOqSnnnpK3bt3108//aSAgAB99tlnGjt2rObMmaNrr71W+/btU9b5P/ctW7borrvu0quvvqpu3brpjz/+0MGDB6+4LgAAAKCmIhyWRVaWFBRU4aexSAouZtuuGyT1lJTcSs2bGjKZTKU6dt++fRUQEKClS5dq1KhRkqStW7fq0KFDuueeeyRJb775pnx8fByfef7559WjRw+dPHlSkZGRpf9C523YsEELFy7Upk2b9Je//EWS1Lp1a3Xq1ElNmzbVBx98oIceekhr1qxRp06ddNddd0mSWrVq5TjG+vXrVb9+fUeQbdmyZZnrAQAAAMC00iprV53zvyTHqFmTohesuRQ/Pz8NHDhQixYtcozNnz9fvXv3Vt26dSVJPj4+OnPmjJYvX66ZM2fqvffekySdOHHiimpfuXKlYmJiHMHQLioqSnFxcVq3bp0kKTY2VuvWrdO0adOUnp7utG/Xrl116NAhjR8/XsnJyVdUDwAAAADCYdkEBEgZGRXz2rjR9kyKU6dUkJai9MT1Sk9cLyM9zWm/3b3OP+U+OUbNr7GU6Wvcc889Wr16tc6ePSvDMLRw4UJHF1GSHn/8cdWrV0/PP/+8/ve//yni/HKoBQWlD6MXO3nypBo2bFjktujoaJ09e1aSNHToUH344Yd699131aBBA02cOFHZ56fddu3aVStWrNA333yjRo0a6cEHH9SZM2euqC4AAACgJmNaaVmYTFJgYMUcOzBQysuT/P1l8veT5C9JMgL8ZTLb/rqsBVbtObPPtv+pVmp+demmlNrdeOONqlu3rv7zn/+oadOmOnfunG6//XZJtu7eG2+8oe3btzumc+7atUuzZs26oq8nSbVr19bmzZuL3HbixAnVqVPH8X748OEaOnSoFi1apEcffVRHjx7V3LlzJUm33nqrbr31Vq1cuVL/+Mc/tHPnTm3YsOGK6wMAAABqIjqHnuaiFUttq5Xag9+Fbt3hlMPKyc+RyeornW1S6pVK7Uwmk4YPH65PP/1UixYt0ogRI+Tt7S1J2rFjhxo0aOB0n9/XX39dthMVEhcXp19//VXff/+903hycrJWr16t3r17O41bLBYNGzZMEydO1LfffutyvFtvvVUvvviiNm7cqLy8vHKpEQAAAKhp6Bx6Gsv5KaJOzzrMl2FceJzF7lO7bb+caikZlit6xuE999yjzp07Kzo6Wp9++qljvH379vrtt9/02muvqWfPnlq/fr0+/PDDsp/oIjfffLP69Omjfv366V//+peuu+46/f7773rqqafUsWNHDR06VJL03HPPKTo6Wp07d1Z2drY+/fRTdevWTZJtsZycnBzdcMMNMpvN+uijj9SlSxdHuAUAAABQOnQOPY29c3g+HF541uGFzuGu5F2SJONkK3l7S8XcvlciMTExuvrqqxUSEqI2bdo4xm+66Sa98MILeuGFF3Tdddfpv//9r955552yn6iQzz77TGPGjNGzzz6rDh066O9//7tuvvlmrVixQl5etn+zuOqqqxyPurjzzjvVrl07vfXWW5Kkxo0b67333tP111+vW265RUFBQU7hFgAAAEDpmAzDMNxdhLscPXpUDRs21JEjR9SgQQOX7Tk5OTp06JCaNGkiPz+/yilq3z4pLU1q3FiKiFBm5q8qKMiRv38LeXmFSJJGLh2pD3/5UFozVdckPa09eyqntJrILdcAAAAAKt3lskFNQOfQ0xTqHNqmlUqGcaFz6JhWmhxT5vsNAQAAAOBihENPU+iewwvTSm33HBqG4ZhWqlOtruh+QwAAAACwIxx6mst0Do+mHVXGuQyZDC/pTHM6hwAAAADKBeHQ01z0KAvJdUEa+5RSn/SrJasP4RAAAABAuSAcehqXaaX2zqEtLNqnlOYn2p4/SDgEAAAAUB4Ih57GZVqp7b19WunuZFvn0HoiRmazdNVVlV0gAAAAgOqIcFgClfq0j2KnlZ7vHJ46vxhNcoyuukry8am80mqiGvykFwAAANQwhMNLsJyf4pmXl1d5J73EgjSFVyplSmnFs//d268FAAAAoLoiHF6Ct7e3fH19lZqaWnkdpEs8yiI5K1lnss9Ihkk6dQ3hsIIZhqHU1FT5+vrK29vb3eUAAAAAFcrL3QV4uoiICB07dkxHjx5VaGiovL29ZTKZKu6E+fkXfubkKC/PqnPnpPz8PP2c/LMkKfBcU2Xm+6tRozzl5FgrrpYayjAM5eXlKTU1VRkZGapfv767SwIAAAAqHOHwMkJCQiRJp06d0rFjxyr+hDk50qlTkpeXZLHIas1SXt4pmc3p+u6P7yRJptMtJElBQSd06FBGxddUQ/n6+qp+/fqOawAAAACozgiHJRASEqKQkBDl5eXJaq3gTt3u3dKDD0qRkdL69UpJ2ah9+x6Uv/81SlZTSdK5Y60lSV27RqpJkzoVW08NZbFYmEoKAACAGoVwWAre3t4VHxiCgqTff5fOnpX8/OTvH6CCgt9ltVq076xtyum5o9dKklq18pWfX8WWAwAAAKBmYEEaTxMYaPuZkSEZhiyWIEmS1ZrhtFJp/fqSv7+bagQAAABQ7RAOPU2QLQyqoEDKzXWEw5ScdJ3IOGHblsxjLAAAAACUL8Khp7F3DiUpM9MRDg9lZEuSQtRQOhdMOAQAAABQrgiHnsZikXx9bb9nZDjC4e9ZtqHArFaSRDgEAAAAUK4Ih57IPrU0M1Nms58ksyMc6mSMJKlZM7dUBgAAAKCa8rhwaLVaNWvWLHXp0uWS+61evVrdu3dXYGCg6tSpo4EDB+rgwYOVVGUFs08tzcyUyWSSxRKk3zNtQ+mHbOGQziEAAACA8uQx4TA7O1tz5sxRmzZtNGHCBOXk5BS7b3p6uoYOHaq+fftqy5YtWrx4sZKSkhQXF6eMjGrwUPiLVyyVbOHwfOcw45BtWimdQwAAAADlyWOec/jDDz9owoQJeuCBB5Sfn69Vq1YVu6+3t7e2bNmiJk2aOMaWLl2qqKgorVmzRn379q2MkivORdNKJSnXCFBS7vltya0UGSmFhLinNAAAAADVk8d0Djt27Kjjx49r2rRpCggIuOS+fn5+TsFQkiIiIhQeHq6TJ09WZJmV46JppZJ0JNsiSQoxh0vZ4XQNAQAAAJQ7j+kcBgcHX9HnDx8+rOTkZLVu3brYfXJzc5Wbm+t4n56efkXnrDCFppX+nmVIkmrlN1WauN8QAAAAQPnzmM7hlcjLy9ODDz6oG2+88ZIL2SQkJCg0NNTxiomJqcQqS6HQtNLDmfmSJJ/UqyURDgEAAACUvyofDpOSknTzzTfrxIkTWrx48SX3nThxolJTUx2vXbt2VVKVpVRoWum206mSpPzjrFQKAAAAoGJU6XC4evVqtWvXTg0bNtSmTZtUp06dS+7v6+urkJAQx+tKp7JWmIumle5I2qGfT5+WxSRl/jRIEiuVAgAAACh/VTYcLly4UHfeeadeeuklffTRR5ddxKZKuWha6Vs/vSVJ+ktts5IPtpBE5xAAAABA+fOYBWlK49ixYxo1apQ+++wz3Xrrre4up/yd7xxmZKXoo18+kyR18Wqo7yTVqiWFhbmvNAAAAADVU5UJh0OGDFHnzp01fvx4LV++XEFBQWrZsqUOHz7stJ+/v7+ioqLcU2R5OR8O55t3Kf1cupqERCj4VFtJtq6hyeTO4gAAAABUR1UmHB46dEj169eXZFuEJjk52eVZh5LUq1cvffPNN5VdXvkKCpIhaXboXknSiJbddOIL23flfkMAAAAAFcEj7zmcMmWKfv75Z6exLVu2aObMmZKkZ555RoZhFPmq8sFQkgIDtbWetD0gTb4WXw1p2UvHjtluNOR+QwAAAAAVocp0DmuUwEC9+Wfbr4OuHaTIoHo6dsy2SA3hEAAAAEBFIBx6oBQ/aX4b2+8PdnxQFkuWjh+3zSclHAIAAACoCB45rbSmm5uyTtneUuuz3urasKus1hAlJV0liXsOAQAAAFQMwqGHMQxDbx5dJkn6+y8+MplMOnq0tgoKLPLzy1Tdum4uEAAAAEC1RDj0MN/98Z12pf+mwHPSiO1WSdLvv4dKkurX/43HWAAAAACoEIRDD/PmT29Kku7eIYWk5khWqw4fti1GU6/efhUU5LmzPAAAAADVFOHQgyRnJmvJriWSpAe2nh/MytLBg36SpHr1fpPVmuGm6gAAAABUZ4RDDzLn5zk6Zz2n6+pdp44nzs8fzczUkSMWSVLduodltaa7sUIAAAAA1RXh0EMUGAV666e3JEkP/vlBKTDQtiEzUykptl9DQs7QOQQAAABQIQiHHuKbg9/o4NmDCvUN1ZBrh1wIhxkZSk21/RoUlELnEAAAAECFIBx6iDe32haiubfdvQr0CZSCbIvQXNw5DAxMpXMIAAAAoEIQDj1AfkG+kjKTJEkPdHzANnjRtNKLO4f5+XQOAQAAAJQ/L3cXAMnL7KWNozdqz6k9ahnR0jZ4Phwa6Remldo6h4RDAAAAAOWPzqEHcQRDyTGtNPNMrqxW+1AK00oBAAAAVAjCoac63zlMOZUvSfLyypevbzadQwAAAAAVgnDoqc6Hw9TTtnAYHJwtk0l0DgEAAAAPsnnzZsXGxiogIEDR0dGaNGmS8vPzi9w3MzNT48aNU926dRUSEqKePXtq69atlVxx8QiHnur8tNKUs4YkKTg4V5LoHAIAAAAeYvfu3YqLi1O3bt20detWvfbaa5o9e7bi4+OL3H/UqFH64osvNGfOHG3YsEFNmzZVr169dPDgwUquvGiEQ09l7xym2MJhaGieJLFaKQAAAOAhpk6dql69eikhIUExMTEaOHCgEhIS9Morrygjw3nGX1pampYsWaKXXnpJvXv3Vrt27fT222+rdu3aWrJkiZu+gTPCoaey33OYapIkhYTYWtNMKwUAAADcz2q1avny5Ro+fLjT+KBBg5STk6ONGzc6jZtMJplMJgXaH1knyWw2KyAgQFb7CpRuRjj0VOenlaam2/6KQkNtFwzTSgEAAICKk56errS0NMcrNze3yP0OHz6sjIwMtW3b1mk8LCxMUVFR2r9/v9N4cHCwRo8erUmTJum3335TTk6Opk2bppMnT+q+++6rsO9TGoRDT2XvHGbaHkUZEmKbXko4BAAAACpOTEyMQkNDHa+EhIQi90tOTpYkhYeHu2wLCwtTWlqay/grr7wiwzDUvHlzBQQE6JlnntG8efNUr1698v0SZeTl7gJQDHvnMNNbklSrlm2YaaUAAABAxdm1a5fq16/veO/r61vkfvYVSc1m136bfQpp4f0HDBggX19frVixQuHh4fr88881ePBgffXVV+rcuXM5fouyIRx6KnvnMNt2MdaqZbvo6BwCAAAAFSc4OFghISGX3c++T2pqqkv3MCUlxWVs3rx52r59u3777TcFnW8EderUSefOndNDDz2kn376qZy+QdkxrdRT2cNhrp8kqXZtW46ncwgAAAC4X7NmzWQ2m7Vnzx6n8dTUVCUmJqp169ZO499//73atm3rCIZ2N9xwg7Zt21bsvY2ViXDoqezTSnP9JUm1atmml/IoCwAAAMD9AgMDFRsbqwULFjiNL1myRJGRkS7TROvVq6edO3cqOzvbaXzz5s0KDw8vdvpqZSIceip75zDP9rN2bR9JUkFBpgyjwG1lAQAAALB5+umnNW/ePCUkJGjPnj1avHixnnjiCU2bNk0Wi0VDhgzRzJkzJUl//etflZ+fr379+mn9+vXasWOHXnjhBc2YMUMTJ0508zexIRx6qvPhMNVqD4d+jk1Wa6ZbSgIAAABwQVxcnObNm6e5c+eqXbt2io+P14wZMzRq1ChJ0qFDh3T06FFJUnR0tH788UdFRkbq7rvvVteuXfXpp5/qo48+0uOPP+7Or+HAgjSe6vy00pQC242uYWG+SkszSyqQ1ZouL69gNxYHAAAAQJIGDx6swYMHF7lty5YtTu+vuuoqffzxx5VRVpnQOfRU9s6hQiVJtWqZZLHYAiGL0gAAAAAob4RDTxUYqDx5KUu2kFirlmSx2LqJPM4CAAAAQHkjHHoqb2+letdxvA0JkWMqKSuWAgAAAChvhEMPluIfLUkKCrDKy0tMKwUAAABQYQiHHizVL0qSVCvYKolppQAAAAAqDuHQg6X42sJhqH+eJDqHAAAAACoO4dCDpXpHSJJqBeRKonMIAAAAoOIQDj1YipctHIb62sOhvXNIOAQAAABQvgiHHizVEiZJquWbJenCaqVMKwUAAABQ3giHHizFVEuSFOplC4f2aaU8ygIAAABAefO4cGi1WjVr1ix16dLlkvsZhqEZM2aoSZMm8vPzU4cOHbRq1apKqrJypBqhkqRaXrZOIQvSAAAAAKgoHhMOs7OzNWfOHLVp00YTJkxQTk7OJfd/7rnn9OKLL2rmzJnatm2bunfvrjvuuEP/+9//KqniipdyPhyGmm2dQhakAQAAAFBRPCYc/vDDD5owYYL69++v8ePHX3Lfs2fP6oUXXtCbb76pAQMGKCYmRi+//LI6duyoF198sZIqrnipVlsYrGVKlcSCNAAAAAAqjseEw44dO+r48eOaNm2aAgICLrnvqlWr5OXlpb59+zqNDxo0SF9//XVFllmpUvICJUmhRookppUCAAAAqDhe7i7ALjg4uMT7/vrrr4qJiZHFYnEaj4mJ0YkTJ5SRkaGgoCCXz+Xm5io3N9fxPj3dsztwqXn+kqRaBWckMa0UAAAAQMXxmM5haSQnJys8PNxlPCzM9uiHtLS0Ij+XkJCg0NBQxysmJqZC67xSKbm2cBhqtYVD+6MsWK0UAAAAQHmrkuEwPz9fZrNr6SaTyelnYRMnTlRqaqrjtWvXrgqt80ql5vhKkmrln5LEtFIAAAAAFcdjppWWRkhIiPbt2+cynpKSIpPJpNq1axf5OV9fX/n6+jreF9dh9ASGIaVmeUuSQnNPSnKeVmoYRrEhGAAAAABKq0p2Dlu0aKE9e/a4jO/Zs0fNmzeXn5+fG6oqX5mZkrXA9tdTKzdJ0oXOoVSggoJLP+oDAAAAAEqjSobDm2++WadOndKaNWucxhctWqR+/fq5qarylZJi++mlPPlnnZYkWSyBju0sSgMAAACgPFWZcDhkyBDNnDlTktS0aVONGDFCo0eP1sqVK7Vr1y49+uij2rt3ryZMmODmSsuHPRzWUopMmbZ7DE0ms8xmW0AkHAIAAAAoT1UmHB46dEhHjx51vH/rrbd0xx13aMSIEbruuuu0a9curV27VlFRUW6ssvyk2p57r1Cl2uaYnmdfsZRFaQAAAACUJ5NhGIa7i3CXo0ePqmHDhjpy5IgaNGjg7nKcfPmldPvtUkdt1VZTJyk/XzKb9cMPVys7+4Dat9+gWrVi3V0mAAAAUC14cjaoLFWmc1jT2DuHtZRiW7o0O1sSj7MAAAAAUDEIhx7Kfs9hqM6nxPNTSy9+nAUAAAAAlBfCoYdydA4t5zuEjnBo7xwSDgEAAACUH8Khh3J0Dn1s00mVYQuJTCsFAAAAUBEIhx7K0Tn0PR8OmVYKAAAAoAIRDj2Uo3Pol2v75Xw45FEWAAAAACoC4dBDOTqHgXm2XxzTSm2dw/x8OocAAAAAyg/h0EM5Oof2cMiCNAAAAAAqEOHQQzk6h0FW2y8u4ZBppQAAAADKD+HQQzk6hyGG7ZdC00rpHAIAAAAoT4RDD+XoHIaeD4dMKwUAAABQgQiHHigvT8rKsv0eWvv8XxGrlQIAAACoQIRDD2TvGkpSSJiX7RdWKwUAAABQgQiHHsh+v2FQkOQV7G97w4I0AAAAACoQ4dADOe43rCUpMND2xhEOWZAGAAAAQPkjHHogx0qlobK1D6WLppXaOoeGcU4FBecqvzgAAAAA1RLh0AOVpHMoMbUUAAAAQPkhHHogp85hoXBoNnvLZPKVxNRSAAAAAOWHcOiBnDqHhaaVSjzOAgAAAED5Ixx6oEt1DiUeZwEAAACg/BEOPZA9HBZ1z6F08eMsCIcAAAAAygfh0APZp5UWtVqpxLMOAQAAAJQ/wqEHKrJzeO6clJ8viWcdAgAAACh/hEMPVOSjLKSLHmdB5xAAAABA+SIceiCnBWl8fCQvL9vA+amlF1YrpXMIAAAAoHwQDj2QU+fQZHJZlIbVSgEAAACUN8KhB3LqHEpFhEOmlQIAAAA10bvvvquMjIrJAYRDD2MYhTqHksuKpSxIAwAAANRM8fHxio6O1ujRo7Vx48ZyPTbh0MNkZEgFBbbf6RwCAAAAuNixY8e0aNEinTt3Trfeeqtatmyp6dOnKykp6YqPTTj0MPauobe35O9/frDYcEjnEAAAAKhJLBaLevfurY8//lhJSUmKj4/Xt99+qyZNmqhfv3764osvZLVay3RswqGHufh+Q5Pp/CDTSgEAAAAUEhAQoBEjRmjFihWaO3eutm/frn79+qlBgwZ65plndPr06VIdj3DoYVzuN5RcwuGFR1kwrRQAAACoqbZs2aLHH39cjRo10t/+9jfdfvvt+v777/Xvf/9bGzduVExMjH766acSH8+rAmtFGbisVCpJISG2n+eTI4+yAAAAAGqm7du3a+HChVq0aJGOHj2qXr166cUXX1T//v3l4+MjSercubMGDx6sGTNmaMyYMdq2bVuJjk049DBFdg7tbxzhkHsOAQAAgJqoY8eOatmypR544AHde++9io6OLnbfPn36aMqUKSU+NtNKPUyRnUP7G5dwyLRSAAAAwJ02b96s2NhYBQQEKDo6WpMmTVJ+fn6x+6ekpGjs2LFq0KCBfH19ddVVV2n9+vUlPt/333+vXbt26Z///Oclg6EkNW/eXHv37i3xsQmHHqbIzqE9HJ5PjvZppQUFWTKMsq1EBAAAAODK7N69W3FxcerWrZu2bt2q1157TbNnz1Z8fHyR+6elpalbt246ePCg5s6dqx07dujdd99VVFRUic85e/ZsTZ06tchtEyZM0LPPPut47+vrqwYNGpT42Ewr9TCl6RxKktWaKS+vkMopDgAAAIDD1KlT1atXLyUkJEiSYmJidOrUKT322GOKj49XkH1hyfMmT56syMhIffHFFzKbbX26Fi1alOqcy5Yt04oVK4rcdtttt2n06NGaPHlyGb4NnUOPU5J7Ds1mX5lMtlzPfYcAAABA5bNarVq+fLmGDx/uND5o0CDl5ORo48aNTuNZWVl69913NWXKFEcwLItz587J3/FAdGcRERFKTEws87E9KhyWZr5uZmamxo0bp7p16yokJEQ9e/bU1q1bK7ni8leSzqHJZGLFUgAAAKACpKenKy0tzfHKzc0tcr/Dhw8rIyNDbdu2dRoPCwtTVFSU9u/f7zS+adMm5efnKzQ0VD169FCtWrXUsmVLzZ49u1T1tWnTRp9//nmR29atW6eGDRuW6ngX85hwWNr5uqNGjdIXX3yhOXPmaMOGDWratKl69eqlgwcPVnLl5ask9xxKLEoDAAAAVISYmBiFhoY6XvYpo4UlJydLksLDw122hYWFKS0tzWls9+7dqlWrlu677z6NHDlSX3/9te6++26NHTtWn3zySYnrGz9+vJ5//nnNnj1bhmE4xpcuXar4+Hj99a9/LfGxCvOYew5LM183LS1NS5Ys0aeffqrevXtLkt5++2198803WrJkiZ588km3fIfyUGTnsNC0UunCojRMKwUAAADKz65du1S/fn3He19f3yL3s89wLGqKqMlkkslkchpLS0vTiRMn9Mknn6hnz56SpOuuu06JiYmaNm2ay/TU4gwePFhHjx7V+PHjNWHCBDVq1EgnTpxQWlqaRo0adUVZyCM6h6Wdr2v/ww4MDHSMmc1mBQQEyGqt2qt3XrJzmJkpnb8I6RwCAAAA5S84OFghISGOV3HhMCTEtihk6kUNHLuUlBSXjqK3t7f8/PzUo0cPp/G4uDjt27dPeXl5Ja5x/Pjx+v333/XGG2/o3nvv1XPPPaf//e9/evfdd11CaWl4ROewJPN1b7nlFsd4cHCwRo8erUmTJqlZs2aqX7++Zs6cqZMnT+q+++4r9jy5ublOc4bT0z2v61Zk5zDkotVIU1Ol8PCLwqHnfQcAAACgumvWrJnMZrP27Nmjpk2bOsZTU1OVmJio1q1bO+3fpEkT5ebmKicnx2lBGbPZLMMwSh3qIiMjL5l9ysIjwmFp5+tK0iuvvKJu3bqpefPmMplMMpvNWrFiherVq1fseRISEpye++GJiuwcentLgYG2zqEjHDKtFAAAAHCXwMBAxcbGasGCBerTp49jfMmSJYqMjFTnzp2d9u/Ro4csFosWL16se++91zG+YsUKde7cWV5eJY9mSUlJ2rBhg06cOKGCggKX7Y888kgZvpGHhMPSztfNz8/XgAED5OvrqxUrVig8PFyff/65Bg8erK+++srlL8Ju4sSJGj9+vOP9sWPHFBMTU47f5MqcOydlZdl+dwqHkq2VaA+Hkry8mFYKAAAAuNPTTz+tW2+9Va1atdKAAQO0Y8cOPfHEE3rppZdksVg0ZMgQde7cWePHj1dERIQeeeQRjR07VoZhqEOHDlq2bJk++ugjrVy5ssTnXLFihQYNGqSCggJZLBb5+fnJYrEoOTlZERERqlWrVtUOhxfP1y3cPSxqvu68efO0fft2/fbbb46Fajp16qRz587poYce0k8//VTkeXx9fZ3mDBfVkXSni6crhxR+rn1oqHT8uGPeKY+yAAAAANwrLi5O8+bN05QpUzRlyhQ1btxYM2bM0KhRoyRJhw4dclrcZvr06QoMDNSkSZOUnJys1q1b6/PPP3e5D/FSJk6cqFGjRmnmzJl66qmn5Ofnp+eff16//PKLHnjgAb366qtl/j5lDofbtm1TQUGB/vznPzvG5syZo2+//VY33HCD7r///hIfq7Tzdb///nu1bdvWaQVTSbrhhhs0Y8YM5ebmFnvjqCezh8PgYMliKbSx0LMOWZAGAAAAcL/Bgwdr8ODBRW7bsmWL03uLxaKpU6dq6tSpZT7fgQMHtGDBAvn4+Kh58+Zav369JKldu3aaNGmSHn74YZfzllSZVysdPny4/ve//znez5kzR3/729+Ulpam+Pj4UiXWi+frXqy4+br16tXTzp07lZ2d7TS+efNmhYeHV8lgKBWzGI1docdZsCANAAAAUPNERUUp5XxwaNWqlXbs2OHY1qxZM+3cubPMxy5zODx8+LAjtBUUFOi5557T1KlT9Z///EcffPCBZs+eXarjPf3005o3b54SEhK0Z88eLV68WE888YSmTZvmmK87c+ZMSdJf//pX5efnq1+/flq/fr127NihF154QTNmzNDEiRPL+pXcrsjFaOzsibHQtFLCIQAAAFBz2KeySlKXLl105MgRzZ07V4mJiZo5c6aaNWtW5mOXeVppVFSU45keS5Ys0ZkzZzR27FhJUuPGjXX48OFSHa8083Wjo6P1448/atKkSbr77ruVmpqqli1b6qOPPtKQIUPK+pXc7pKdQ6aVAgAAADVefHy8vvjiC+Xn58vX11ezZ8/WX//6V+Xk5CgwMFCLFy8u87HLHA6HDRumBx98UEOGDNHrr7+uRx991HEP4N69exUWFlbqY5Zmvu5VV12ljz/+uPSFe7BLdg4LTSu9sFopnUMAAACgpmjUqJEefvhhx/thw4apd+/eOnDggJo3b65aRYaJkinztNKpU6eqe/fuWrBggfr166f4+HjHts8++6zYkIfila5zaJ9WSucQAAAAqAkKCgpUv359/fjjj07jtWrV0p///OcrCobSFXQOvb299corrxS57YMPPijrYWu00t1zaOsc8igLAAAAoGYwm83y8/NzPCe+3I9f1g8ePXpUv//+u9PYmjVrNHnyZH3zzTdXXFhNVLrVSlmQBgAAAKhp3nnnHT311FP6+eefy/3YZQ6Hd9xxh5YtW+Z4/8UXXyguLk6ff/65BgwY4PJYClxeiTqHLEgDAAAA1FiPP/64jh07po4dOyo6Olrt27fXn/70J6dXWZV5WunevXt14403Ot4/9dRTevTRRzVz5kwtXrxYCQkJGjp0aJkLq4lKdM9hoWmlVmuGDMOQyWSq8PoAAAAAuFf//v0r7NhlDoe1a9eW1WqVJK1atUoHDhzQ2rVrJUnt2rXTvn37yqfCGqR0ncOg8xsKVFCQJYslsKLLAwAAAOBmkydPrrBjl3laab9+/fT444/rk08+0dixYzVmzBhFRERIsj2TMDCQsFJapbvnMECSrVvI1FIAAAAAV6rMncOEhATdc889evDBBxUbG6tp06Y5tr311lvq0aNHuRRYk5Soc5ibK+XkyOTnJ4slUFZrhvLz0+XjE1VZZQIAAABwkzvvvPOy+3z22WdlOnaZw2FoaKg+//zzci2mprtk5zA4WDKZJMOwpUg/P1kswbJaM+gcAgAAADVEaBFhISMjQz/++KMyMzN1xx13lPnYZQ6HdikpKfrhhx909uxZ1alTR3/5y18UEBBwpYetceyZTyqmc2g2SyEhtp1SU6WoqPOL0iTyOAsAAACghpgzZ06R41arVY8++qiaNGlS5mOX+Z5DSZoyZYrq1aun3r17a9SoUbrpppvUoEEDvf/++1dy2BopI0MqKLD9XmTn8OINjhVLedYhAAAAAMlisehf//qXXn/99TIfo8zh8LXXXtP06dM1Y8YMnTlzRtnZ2Tp79qz+3//7fxo3bpzTMxBxefauobe35O9fzE486xAAAADAJZw5c6bMny3ztNI33nhD/+///T89/PDDjrHQ0FD9/e9/V25urhISEtSvX78yF1bTXHy/YbGPLHRZsZTOIQAAAADpyJEjevLJJ9W+ffsyH6PM4fDgwYOKi4srcluvXr301FNPlbmomuiS9xvaFeocennROQQAAABqktq1a8tUqJuUk5Oj3NxcNW3a9IpmcF7RaqWnTp0qcltycrJ8fX3LXFRNZO8cligcOu45tIXD/Hw6hwAAAEBN8PLLL7uEQ39/fzVs2FCdOnWSxWIp87HLHA5vuukmJSQkqHv37k7FGYahhIQEnnNYSvbOYbGL0Vy8kWmlAAAAQI00cuTICjt2mcPh888/r86dO6tt27YaOXKkGjZsqGPHjmnOnDk6evSoNm3aVJ51Vnsl6hy63HPItFIAAACgJpk0aZLCw8M1fvx4l23PPvus6tatqwceeKBMxy7zaqWNGzfWjz/+qE6dOunll1/WiBEj9MILL6hNmzbasmWLWrZsWdZD10gXL0hTrGKmldI5BAAAAGqGd955R3/605+K3GbPZmVV5s6hJDVq1EjvvffelRwC55VlQZoL00rpHAIAAAA1QXp6uiIiIorc1qhRI/3+++9lPnaJw+Gdd95Z6oN/9tlnpf5MTVWizmGhaaUXViulcwgAAADUBC1atNDatWvVunVrl20//fSTIiMjy3zsEofD0EumFlypUnUOHdNKbZ1DVisFAAAAaoYHHnhA//d//6fmzZurd+/ejvFt27bp//7v/3TvvfeW+dglDodz5swp80lweaW655AFaQAAAIAa6aGHHtKePXt022236eqrr1bTpk2VmJioX3/9VT179tSzzz5b5mOXeUEalK8SdQ6LXa2UziEAAABQU7zyyivatm2bhg8frkaNGikuLk7Lli3TqlWrruh581e0IA3KT5s2kmFI0dGX2OnizqFh8JxDAAAAoIZq37692rdvX67HJBx6iLffLsFO9nBotUqZmbJ4Ma0UAAAAqElGjhyppk2b6plnnnHZNmHCBAUHB2vy5MllOjbTSquSgADJYrH9nprq6BwaRp4KCnLdWBgAAACAyrBs2TLddNNNRW677bbb9MEHH5T52ITDqsRkcrrv0B4OJbqHAAAAQE1w7tw5+fv7F7ktIiJCiYmJZT424bCquehxFmazl8xm24XB4ywAAACA6q9Nmzb6/PPPi9y2bt06NWzYsMzHJhxWNS6Ps2BRGgAAAKCmGD9+vJ5//nnNnj1bhmE4xpcuXar4+Hj99a9/LfOxWZCmqinicRZ5eclMKwUAAABqgMGDB+vo0aMaP368JkyYoEaNGunEiRNKTU3V6NGj9eSTT5b52ITDquaiaaUSzzoEAAAAaprx48drxIgRWrlypY4dO6bg4GD16NFD11577RUdl3BY1RQ7rZTOIQAAAFATJCUlacOGDUpLS1NgYKAKCgq0evVqrV69WpL0yCOPlOm4hMOqplA49PKicwgAAADUFCtWrNCgQYNUUFAgi8UiPz8/WSwWJScnKyIiQrVq1SpzOGRBmqrGfs+hY1qprXPIaqUAAABA9Tdx4kSNGjVKqampevDBB/Xggw8qKSlJ27dvV9OmTfXJJ5+U+diEw6rGZVqpvXPItFIAAACgujtw4IAefvhh+fj4qHnz5jp48KAkqV27dpo0aZIefvjhMh+bcFjVFBsO09xVEQAAAIBKEhUVpZTzswhbtWqlHTt2OLY1a9ZMO3fuLPOxCYdVTaFppV5etSVJ+fln3VMPAAAAgEoTFxenefPmSZK6dOmiI0eOaO7cuUpMTNTMmTPVrFmzMh+bBWmqmkKdQ2/vcElSXt5pd1UEAAAAoJLEx8friy++UH5+vnx9fTV79mz99a9/VU5OjgIDA7V48eIyH5twWNUQDgEAAIAaq1GjRk73FQ4bNky9e/fWgQMH1Lx5c9WyzzQsA4+aVrp582bFxsYqICBA0dHRmjRpkvLz84vdPyUlRWPHjlWDBg3k6+urq666SuvXr6/Eit3A/pdNOAQAAAAgqVatWvrzn/98RcFQ8qDO4e7duxUXF6exY8fq7bff1u7duzVmzBhZrVa98MILLvunpaWpW7duatSokebOnav69evr999/V1RUlBuqr0T2zmFammS1ysvLFg7z8wmHAAAAAMrOY8Lh1KlT1atXLyUkJEiSYmJidOrUKT322GOKj49XUFCQ0/6TJ09WZGSkvvjiC5nNtgZoixYtKr3uSmcPh5KUni5vX3vn8JSbCgIAAABQHXjEtFKr1arly5dr+PDhTuODBg1STk6ONm7c6DSelZWld999V1OmTHEEwxrD19f2kqTUVMe00oKCHFmtWW4sDAAAAEBV5hHJ6vDhw8rIyFDbtm2dxsPCwhQVFaX9+/c7jW/atEn5+fkKDQ1Vjx49VKtWLbVs2VKzZ8++5Hlyc3OVlpbmeKWnp5f7d6kUFz3OwmIJlslkawBz3yEAAACAsvKIcJicnCxJCg8Pd9kWFhamtDTnB7zv3r1btWrV0n333aeRI0fq66+/1t13362xY8fqk08+KfY8CQkJCg0NdbxiYmLK94tUlotWLDWZTPL2jpBEOAQAAABQdh5xz6F9RdKipoiaTCaZTCansbS0NJ04cUKffPKJevbsKUm67rrrlJiYqGnTprlMT7WbOHGixo8f73h/7NixqhkQCz3OwssrXOfOnWBRGgAAAABl5hGdw5CQEElS6vmwc7GUlBSXjqK3t7f8/PzUo0cPp/G4uDjt27dPeXl5RZ7H19dXISEhjldwcHA5fYNKdtG0UonHWQAAAAC4ch4RDps1ayaz2aw9e/Y4jaempioxMVGtW7d2Gm/SpIlyc3OVk5PjNG42m2UYhkunsdop1DkkHAIAAAC4Uh4RDgMDAxUbG6sFCxY4jS9ZskSRkZHq3Lmz03iPHj1ksVi0ePFip/EVK1aoc+fO8vLyiNmyFYdwCAAAAKCceUyKevrpp3XrrbeqVatWGjBggHbs2KEnnnhCL730kiwWi4YMGaLOnTtr/PjxioiI0COPPKKxY8fKMAx16NBBy5Yt00cffaSVK1e6+6tUvELTSr28bOGQew4BAAAAlJXHhMO4uDjNmzdPU6ZM0ZQpU9S4cWPNmDFDo0aNkiQdOnRI9evXd+w/ffp0BQYGatKkSUpOTlbr1q31+eefu9yHWC0V2zk85a6KAAAAAFRxHhMOJWnw4MEaPHhwkdu2bNni9N5isWjq1KmaOnVqZZTmWZhWCgAAAKCcecQ9hyglwiEAAACAckY4rIqKueeQcAgAAABUrs2bNys2NlYBAQGKjo7WpEmTHM9xv5TU1FSFhYWpf//+FV9kCREOqyKXzmGEJBakAQAAACrT7t27FRcXp27dumnr1q167bXXNHv2bMXHx1/2sy+++KLOnj1bCVWWnEfdc4gSKmZaaX5+igoK8mU289cKAAAAVLSpU6eqV69eSkhIkCTFxMTo1KlTeuyxxxQfH6+goKAiP3fgwAHNnj1bN954Y2WWe1l0Dqsil2mltR2b8vM9618fAAAAgOrIarVq+fLlGj58uNP4oEGDlJOTo40bNxb72QceeEATJkxQ48aNK7jK0iEcVkX2zmF2tpSXJ7PZS15etSRx3yEAAABwJdLT05WWluZ45ebmFrnf4cOHlZGRobZt2zqNh4WFKSoqSvv37y/yc7NmzVJycrIef/zxcq/9ShEOq6KQkAu/n59aal+UhvsOAQAAgLKLiYlRaGio42WfMlpYcnKyJCk8PNxlW1hYmNLS0lzGt23bpmeffVbz58+Xt7d3+RZeDrg5rSry8pKCgqSMDNvU0ogIeXuHKyfnNzqHAAAAwBXYtWuX6tev73jv6+tb5H72FUnNZtd+m8lkkslkcho7ffq07rrrLk2fPl3XXnttOVZcfgiHVVVoqC0cujzr8JQ7qwIAAACqtODgYIVcPFOvGPZ9UlNTXbqHKSkpTmN5eXkaOHCgrr/+eo0ZM6Z8Cy5HhMOqKjRUOnasiHBI5xAAAACoaM2aNZPZbNaePXvUtGlTx3hqaqoSExPVunVrx9imTZu0bt06SdLHH3/sciyTyaS1a9eqe/fuFV73pRAOqyr7ojSOFUsJhwAAAEBlCQwMVGxsrBYsWKA+ffo4xpcsWaLIyEh17tzZMdaxY0dt377d5RjPPPOM0tPT9fLLL6t58+aVUvelEA6rKvvjLFyedUg4BAAAACrD008/rVtvvVWtWrXSgAEDtGPHDj3xxBN66aWXZLFYNGTIEHXu3Fnjx49X+/btXT4fFhYms9lc5DZ3YLXSqsreOXSEwwhJdA4BAACAyhIXF6d58+Zp7ty5ateuneLj4zVjxgyNGjVKknTo0CEdPXrUzVWWHJ3DqsolHDKtFAAAAKhsgwcP1uDBg4vctmXLlkt+9oMPPqiAisqOzmFVZZ9Wev6eQ8IhAAAAgCtBOKyqCnUO7QvScM8hAAAAgLIgHFZVl5hWahiGu6oCAAAAUEURDquqYqaVGkaerNZ099QEAAAAoMoiHFZVhTqHFkuAzGY/Sdx3CAAAAKD0CIdVVaFwKF2475BwCAAAAKC0CIdVlT0cnp9WKl2YWsqiNAAAAABKi3BYVdnvOUxNlc4vQMPjLAAAAACUFeGwqrJ3DvPypJwcSZK3d8T5IcIhAAAAgNIhHFZVQUGSyWT7vdCKpYRDAAAAAKVFOKyqzGaXRWnsC9JwzyEAAACA0iIcVmWFwiGdQwAAAABlRTisygiHAAAAAMoJ4bAqs69Y6nLP4Sn31AMAAACgyiIcVmXF3HNI5xAAAABAaREOq7JippWyIA0AAACA0iIcVmX2cFhoWqnVmqGCgnNuKgoAAABAVUQ4rMrs9xw6ppXWkv2vlKmlAAAAAEqDcFiVFZpWajKZ5e0dJolwCAAAAKB0CIdVWaFppdKFRWm47xAAAABAaRAOq7JC00olnnUIAAAAoGwIh1VZoWmlEuEQAAAAQNkQDquyIqaVXgiHp9xQEAAAAICqinBYlRXRObTfc0jnEAAAAEBpeFQ43Lx5s2JjYxUQEKDo6GhNmjRJ+fn5l/1camqqwsLC1L9//4ov0pPY7zlMS5MMQ9KFziEL0gAAAAAoDY8Jh7t371ZcXJy6deumrVu36rXXXtPs2bMVHx9/2c+++OKLOnv2bCVU6WHsncOCAikjQxL3HAIAAAAoGy93F2A3depU9erVSwkJCZKkmJgYnTp1So899pji4+MVFBRU5OcOHDig2bNn68Ybb6zMcj2Dv7/k5SXl59vuOwwOJhwCAAAAKBOP6BxarVYtX75cw4cPdxofNGiQcnJytHHjxmI/+8ADD2jChAlq3LhxBVfpgUwml8dZcM8hAAAAgLLwiHB4+PBhZWRkqG3btk7jYWFhioqK0v79+4v83KxZs5ScnKzHH3+8ROfJzc1VWlqa45Wenn7FtbtdoUVpvL0jJHHPIQAAAIDS8YhwmJycLEkKDw932RYWFqa0tDSX8W3btunZZ5/V/Pnz5e3tXaLzJCQkKDQ01PGKiYm5ssI9QaHHWVyYVnpGhlHgpqIAAAAAVDUeEQ7tK5Kaza7lmEwmmUwmp7HTp0/rrrvu0vTp03XttdeW+DwTJ05Uamqq47Vr164rK9wTFJpWag+HUoHy81OL/AgAAAAAFOYR4TAkJESS7ZEUhaWkpDh1FPPy8jRw4EBdf/31GjNmTKnO4+vrq5CQEMcrODj4ygr3BIWmlZrNPrJYbIv3cN8hAAAAgJLyiNVKmzVrJrPZrD179qhp06aO8dTUVCUmJqp169aOsU2bNmndunWSpI8//tjlWCaTSWvXrlX37t0rvG6PUGhaqWRblMZqzVBe3ilJzd1SFgAAAICqxSPCYWBgoGJjY7VgwQL16dPHMb5kyRJFRkaqc+fOjrGOHTtq+/btLsd45plnlJ6erpdfflnNm9egQFS7tu3nmTOOIW/vcOXm/s6iNAAAAABKzCPCoSQ9/fTTuvXWW9WqVSsNGDBAO3bs0BNPPKGXXnpJFotFQ4YMUefOnTV+/Hi1b9/e5fNhYWEym81FbqvW6tWz/Tx2zDHEsw4BAAAAlJZH3HMoSXFxcZo3b57mzp2rdu3aKT4+XjNmzNCoUaMkSYcOHdLRo0fdXKUHatTI9vOPPxxDhEMAAAAApeUxnUNJGjx4sAYPHlzkti1btlzysx988EEFVFQFNGxo+3nkiGPIy4twCAAAAKB0PKZziDKydw6PHZOsVkkXOofccwgAAACgpAiHVV3dupKXly0YJiZKkry9IyTROQQAAABQcoTDqs5ikerXt/1+fmop9xwCAAAAKC3CYXVgv+/w/KI0hEMAAAAApUU4rA4KLUpzYUGaU+6qCAAAAEAVQzisDgo9zoIFaQAAAACUFuGwOijUObSHw4KCHFmtWe6qCgAAAEAVQjisDgp1Di2WYJlMtkdYct8hAAAAgJIgHFYHhTqHJpPpovsOCYcAAAAALo9wWB3YO4fJyVJ2tiTuOwQAAABQOoTD6qB2bSkgwPb70aOSeJwFAAAAgNIhHFYHJlMRi9JESCIcAgAAACgZwmF1UczjLAiHAAAAAEqCcFhdFOoc2hek4Z5DAAAAACVBOKwuiu0cnnJXRQAAAACqEMJhdeFyzyHTSgEAAACUHOGwurB3DgmHAAAAAMqAcFhd2DuHf/whGYbjnkPCIQAAAICSIBxWF/ZwmJEhpaY6OocsSAMAAACgJAiH1UVAgBRuC4T644+LwmGKCgry3VgYAAAAgKqAcFidXLQojZdXmGM4P/+smwoCAAAAqrfNmzcrNjZWAQEBio6O1qRJk5SfX3RzZvXq1erevbsCAwNVp04dDRw4UAcPHqzkiotHOKxOLnqchdnsJS+vWpK47xAAAACoCLt371ZcXJy6deumrVu36rXXXtPs2bMVHx/vsm96erqGDh2qvn37asuWLVq8eLGSkpIUFxenjIwMN1TvysvdBaAcFXqchZdXuPLzU7jvEAAAAKgAU6dOVa9evZSQkCBJiomJ0alTp/TYY48pPj5eQUFBjn29vb21ZcsWNWnSxDG2dOlSRUVFac2aNerbt2+l118YncPqhMdZAAAAAJXCarVq+fLlGj58uNP4oEGDlJOTo40bNzqN+/n5OQVDSYqIiFB4eLhOnjxZ4fWWBOGwOrn4cRa6OByecldFAAAAQJWSnp6utLQ0xys3N7fI/Q4fPqyMjAy1bdvWaTwsLExRUVHav3//Zc91+PBhJScnq3Xr1uVS+5UiHFYnhaaV0jkEAAAASicmJkahoaGOl33KaGHJycmSpHD7EwMuEhYWprS0tEueJy8vTw8++KBuvPFGdenS5coLLwfcc1id2KeVHj0qFRTIy4twCAAAAJTGrl27VL9+fcd7X1/fIvezr0hqNrv220wmk0wmU7HnSEpK0tChQ3X27Fl9/fXXV1hx+aFzWJ3UqyeZzVJenpSUdNGzDgmHAAAAQEkEBwcrJCTE8SouHIaEhEiSUlNTXbalpKQU2VGUbI+zaNeunRo2bKhNmzapTp065Vf8FSIcVideXraAKEl//CFfX9u/eOTkHHZfTQAAAEA11KxZM5nNZu3Zs8dpPDU1VYmJiUXeR7hw4ULdeeedeumll/TRRx8pICCgssotEcJhdXPRfYeBgddKkjIzd7qxIAAAAKD6CQwMVGxsrBYsWOA0vmTJEkVGRqpz585O48eOHdOoUaO0cOFClxVOPQX3HJaQYRiyWq2OucUeq3176fhx6dQpmc09ZDZfpfx8KT09Ud7etd1dHa6At7e3LBaLu8sAAADAeU8//bRuvfVWtWrVSgMGDNCOHTv0xBNP6KWXXpLFYtGQIUPUuXNnjR8/XsuXL1dQUJBatmypw4cPOx3H399fUVFR7vkSFyEcXoZhGEpJSVFycrKsVqu7y7m8ESOkvn2l4GDpyEmFhLwjw7Dqjz9OyGxOcXd1uEK1atVS3bp1L3mDMwAAACpHXFyc5s2bpylTpmjKlClq3LixZsyYoVGjRkmSDh065FjcJikpScnJyS7POpSkXr166ZtvvqnU2otiMgzDcHcR7nL06FE1bNhQR44cUYMGDYrcJzExUSkpKY4bUr28vDz7P8xPnZJOnJBCQqRGjZSdfVgFBRny9o6Wj0/RN8XC8xmGoaysLJ08eVK1atVSdHS0u0sCAACoVkqSDao7OoeXYLValZqaqjp16igiIsLd5ZRMYKDtp9Uq+flJClJeXoa8vfPl5+fn1tJwZfz9/SVJJ0+eVGRkJFNMAQAAUK5YkOYS8vLyZBiGAu2Bqyrw8bH9PHdOkmSx2AJFQUG2uypCObKvaJWXl+fmSgAAAFDdEA5LwKOnkRZmD4d5eVJBgcxmWzi0WnNUg2cQVxtV6loEAABAlUI4rG68vCR7gMjLk9lsn0qaL8Pw8JVWAQAAALgN4bC6MZmcppaaTGaZTL6SmFoKAAAAoHgeFQ43b96s2NhYBQQEKDo6WpMmTSr2uYKrV69W9+7dFRgYqDp16mjgwIE6ePBgJVfsodx03+EXX3yhOnXq6MiRIxV6HgAAAADlz2PC4e7duxUXF6du3bpp69ateu211zR79mzFx8e77Juenq6hQ4eqb9++2rJlixYvXqykpCTFxcUpIyPDDdV7mELh0H7fYeFwmJeXp48//ljHjx8vl9OGh4erXbt28vX1LZfjAQAAAKg8HvMoi6lTp6pXr15KSEiQJMXExOjUqVN67LHHFB8fr6CgIMe+3t7e2rJli9MDJJcuXaqoqCitWbNGffv2rfT6PUox4dBqdQ6Hx44d0z333KPt27erXr16V3zarl27esTDOwEAAACUnkd0Dq1Wq5YvX67hw4c7jQ8aNEg5OTnauHGj07ifn59TMJSkiIgIhYeH6+TJkxVer8e7ROewLCuWFhQUlFtp1YHVanV3CQAAAEC584hwePjwYWVkZKht27ZO42FhYYqKitL+/ftLdIzk5GS1bt262H1yc3OVlpbmeKWnp19x7R7JJRz6SjJJKpBh2MZGjhzpCNgdOnSQyWTSt99+K0lq3LixZs2apQkTJigoKEhPPfWUJGn58uXq2bOnIiIiFBYWpjvvvFOJiYmO0y5dutTpUQvffvutTCaTfvvtNw0bNkwhISFq2LChnn/++RKF1B07dmjo0KFq2LChgoKC1KVLF23evNlpH8Mw9MYbb6ht27by8/NTRESEJk6c6Niel5en559/Xtdcc418fX0VHR2tV199VZI0ZcoUtW/f3uW83bt316OPPup4P3LkSPXv31/z589X3bp1df31119xfQsWLJCXl5fLP2acPn1aPj4+WrVq1WX/fAAAAIDy5BHhMDk5WZLtnrXCwsLClJaWdsnP5+Xl6cEHH9SNN96oLl26FLtfQkKCQkNDHa+YmJgy1WsYUmam57xcclahcGgymR2PtCgoyJEkvfjii9qwYYMk6csvv9ShQ4ec/uwWLFig3Nxcff/99xo1apQk6c0339TAgQP17bff6vPPP9fu3bv10EMPXfbP695771W3bt20ceNGjR07Vs8884wWLVp02c/NmzdPzZo106effqpNmzapXr16uvPOO5WdfWF67Lhx4/Tkk09q9OjR2rJlixYtWuS4jgzD0F133aV///vf+uc//6mffvpJ7777bpnuifzjjz80Z84cffHFF3rjjTeuuL7+/fsrODhYS5YscTrPokWLFB0drbi4uFLXCAAAAFwJj7jn0L4iqdnsmlVNJtMlH/ydlJSkoUOH6uzZs/r6668veZ6JEydq/PjxjvfHjh0rU0DMypIuugXS7TIypMDAiwbs4dBqtb0sFpnNfiooyJbVmi0vr1BFRESoQYMGkqR69eqpcePGTsfMyspydNjsPvvsM/nYjy3pn//8px5++OHL1jd48GBHiGzTpo1WrVqlxYsXa8iQIZf83LPPPut0vpkzZ6pJkybasWOHOnXqpJ9//lmvvvqqvvjiC91+++2O/Xr27ClJWrZsmb788ktt3brV0SG8VGf5Un799Vf98ccfqlu3brnVN2TIEC1cuNApYH/88ccaNWpUkf9bAAAAACqSR4TDkJAQSVJqaqpL9zAlJaXIjqJke5zF8OHDdfPNN+vLL79UQEDAJc/j6+vr1DW6XEeyyrJYbC+r1dY99Pc/f9/h2RI/zqJ3794uYz4+PkpMTNT333+v/fv3a+PGjcrKylJaWprj77Aod9xxh9P79u3bO7qWl+Lj46P09HRt2rRJe/fudUwvPnHihCRb+GvevLlT8LrYsmXL1KNHjyKnjpZWhw4dnIJhedQ3cuRIXX/99Tp+/Ljq1aunQ4cOafPmzfrkk0+uuF4AAACgtDwiHDZr1kxms1l79uxR06ZNHeOpqalKTEwsstuzcOFC/e1vf9Mbb7zhspBNRQsIsHXrPEWRmdjHR8rOLhQOS/6sw6ioKKf3eXl5uu+++7R06VJdd911atGiherUqXP+mJdesCYsLMzpfVBQkHJzcy9bw8yZMzVp0iS1atVKMTExjmvDfr5jx445XS+FXW57aRT+8yiP+rp06aIWLVpo8eLFGjdunD755BP17NnTpYsLAAAAVAaPCIeBgYGKjY3VggUL1KdPH8f4kiVLFBkZqc6dOzvtf+zYMY0aNUqfffaZbr311souVyZToWmcnujicKiLVyzNkWEYl5yqa9vfeVrje++9pzVr1ujgwYOODtqKFSv0/vvvV0Dx0q5duzRhwgStWbNG3bt3l2Sb6vrcc8859gkODnZaEKewy2339/dXTk6Oy3hRz8os/OdRHvVJtu7hwoULHeFw8uTJl9wfAAAAqCgec2PT008/rXnz5ikhIUF79uzR4sWL9cQTT2jatGmyWCwaMmSIZs6cKcm2amZQUJBatmypw4cPO72SkpLc/E08xCVWLC0osHXtvL29JalEXbwdO3aoTZs2TlMrL3eP55XYuXOnvLy8dOONNxZ7vh49eujXX3/V1q1bizxGjx499M033+jo0aNFbm/YsKGOHDmirKwsx9jZs2e1Z8+eSqlPku655x79+OOP+s9//qOkpCQNGDDgsucGAAAAKoLHhMO4uDjNmzdPc+fOVbt27RQfH68ZM2Y4Vso8dOiQ4z/yk5KSlJycrCZNmri8KnuKqcdyWbHU5NQ9lKS6desqODhYH3/8sXbu3HnJZ0Ta7xOcP3++fv31VyUkJFTo4xbatGmjgoICxcfHa9euXZo/f77+9a9/OXXwbrvtNvXq1Uu33367PvzwQ/3666/673//qylTpkiSRo8erWbNmqlHjx769NNPtXPnTn366aeaNWuWJNt9lV5eXoqPj9e5c+eUkpKiMWPGlGg10/KoT7ItBhQXF6dHHnlEw4cPL9NKqgAAAEB58JhwKNlWtdy1a5dyc3O1d+9e3X///Y5tW7ZscXQOn3nmGRmGUeTrm2++cVf5nqVQOJTkct+hxWLR66+/rsWLF+v666/XsWPHij3c/fffr4cffljjxo1T165dtXfvXr300ksVVn7Lli31/vvva+HCherYsaPeeustffjhh07TYU0mkz7//HPde++9io+PV8eOHTVu3DjH/YEBAQH69ttv1aNHDz300EP685//rGeffVYNGzaUJNWuXVvLly/X+vXrFRkZqU6dOqlfv35q06ZNpdRnd9999+no0aMaPXp0efzRAQAAAGViMkryNPJq6ujRo46phfbHOlwsJydHhw4dUpMmTeTn5+eGCq9Aerq0d6/k6yudDzu5uYk6d+6YvLzC5O9fPgu14MpNnz5dCxYs0LZt2y67b5W+JgEAADzY5bJBTeBRnUOUo4s7h+fzv8VSuhVLUfGsVqveeecd/f3vf3d3KQAAAKjhPGK1UlSA84vNyDCk/HzJ27vQiqUFMpn4twF3SU1N1bFjx/TOO+/IbDbrvvvuc3dJAAAAqOFIB9WV2XwhIDoWpfGR7a/ccKxYCvf49ddf1bFjR23dulXLly+Xj73TCwAAALgJncPqzMdHysuzhcPAQMeKpQUFmSooyHZMM0Xlu/7665WdzfReAAAAeA46h9VZCVYsBQAAAACJcFi9FREOWZQGAAAAQFEIh9WZPRxeNH3R3jm0WnPcUREAAAAAD0U4rM5CQ20/09Js9x5KMpttz8YzDNuKpQAAAAAgEQ6rNz8/KSDA9ntKiiTJZPKWZJFke6QFAAAAAEiEw+qvdm3bz7NnJUkmk4n7DgEAAAC4IBxWd/Zw6DS1lHAIAAAAwBnhsLorYmrphUVpriwcfvvttzKZTEo5f1wAAAAAVRfhsCawdw/PnJF0ceeQew4BAAAA2BAOa4KwMNvP9HQpL++iFUtzZRhWNxYGAAAAwFMQDmsCX98LU0vPnpXZ7H1+1dKa3T00DEOGYbi7DAAAAMAjEA5rCnv38PyqpUOGPKaBA8e53Hc4efJkxcTESJK+++473X777apbt65CQkIUFxenvXv3lvrUVqtVs2bNUocOHRQcHKz69evr8ccfV975BXLsfvvtNw0bNkx16tSRn5+f2rZt63S+bdu26fbbb1etWrUUEBCgzp076/Tp05Jsq7AuXbrU6XiF74k8fPiwTCaT1q1bpz59+sjHx0dfffXVFdfXqlUrjR071uV7jx8/Xl27di31nxcAAADgDl7uLqAqMgxDWXlZ7i7DIcA7QCaT6dI71a4tHT3qmFo6bNhdGj16nE6fTlR0dIRjt/nz52vMmDGSpPfff1+xsbF6/vnnZbVaNW7cOA0dOlTbt28vVX3p6elasGCBJk2apDZt2uiXX37R6NGj1ahRI40bN06SdODAAXXu3FkdOnTQvHnzFBUVpXXr1iknx9bZ3LRpk3r16qX+/fvr888/V3BwsFasWOES4EoiPj5eI0aM0PTp0xUeHn7F9d13333697//rX//+98ym23/3lJQUKAFCxboueeeK3V9AAAAgDsQDssgKy9LQQlB7i7DIWNihgJ9Ai+9k6+vFBgoZWZKZ8+qb987FBDwTy1b9oUefLCNJGnr1q06dOiQ7rnnHknSm2++KR8fH8chnn/+efXo0UMnT55UZGRkiesLCQnRxo0bZbFYJEnXXHONVqxYoa+++soRviZMmKBmzZpp1apVjoDVtm1bxzEeeugh3XbbbZo/f75jrEOHDiWu4WIdOnTQAw884HhfUFBwRfXVqVNH8fHxWrdunXr06CFJWr16tdLS0jRkyJAy1QgAAABUNsJhTVK7tiMcBl3dWH379tSnny7XX//6iLy8gjR//nz17t1bdevWlST5+PjozJkz2rRpk/bt2+foGJ44caJU4dAepnbv3q2ffvpJBw4c0C+//OLodp47d04rV67Ue++959j3Yr///rt++eUXvf7661f6JyBJ6t27d7nWV69ePcXFxWnhwoWOcPjJJ59o8ODBCgrynH9EAAAAAC6FcFgGAd4BypiY4e4yHAK8A0q240VTS81Ws4YPH6Y77rhHSUm7Va/en7Vw4UK9+uqrjt0ff/xxvf7662rfvr1atmypiAjb9NOCgoJS1ffHH3/ozjvv1OHDh9WpUyc1a9ZM4eHhOnXqlCTp1KlTys3NVdOmTYv8/LFjxySp2O2lFRUVVa71SdLIkSM1duxYvfbaa8rLy9Nnn32m//73v+VSLwAAAFAZCIdlYDKZLj+N0xMVmloaF9dfUVHhWrbsC11zzSmdO3dOt99+uyRp5cqVeuONN7R9+3a1atVKkrRr1y7NmjWr1Kd98sknFR4ero0bN8rX19cxtmrVKklydNcSExOL/HxwcLBje3R0dJH7+Pn5Oe5PtMvIKDrAF+7+XWl9ktS/f3/9/e9/15o1a3T27FnVq1dP119/fbH7AwAAAJ6G1Uprmtq1bT/PnpXF4qehQ+/UsmVrtHDhXI0YMULe3rZHXOzYsUMNGjRwBENJ+vrrr8t0yh07duiGG25wBK+CggKtXr3asT0kJEQdO3bUBx98UOTnY2JiFBUVVex2SWrYsKHLSqqbNm2qlPokWzgdOnSoFi5cqE8++USjR48u0bkBAAAAT0HnsKa5eNXSc+d0331/01/+cr1+++2IFi9e5Nitffv2+u233/Taa6+pZ8+eWr9+vT788MMynbJ9+/b64IMPFBsbq6CgIL388svKyMiQv7+/Y5/p06frlltu0T333KMHHnhAQUFBWrlypW666SZ17NhR06dP16hRo2SxWDRs2DCZTCZ9+umneuSRR1SvXj0NHTpUb775pgYOHKhrr71WK1eu1Lx58yqtPsk2tfT2229XTk6O3n777TL9WQEAAADuQuewprFPLZWklBS1bt1ezZo1UUhIoK65prZjt5tuukkvvPCCXnjhBV133XX673//q3feeadMp5w5c6Zat26tO+64Q7fffrtat27tWBHVrmfPnlq9erWOHDmim2++WT169NB3333nuD/w3nvv1ZIlS/Tdd9/phhtuUJ8+ffT77787ppxOnDhRAwcO1E033aQ6derovffe0/Tp0yutPknq3LmzwsPD1atXL8eiPgAAAEBVYTIMw3B3Ee5y9OhRNWzYUEeOHFGDBg1ctufk5OjQoUNq0qSJ/Pz83FBhBTlxwtY9DAqSWrZUQUGOMjN/lST5+7eUlxcrbJZFTk6O6tevrzlz5qhv374Vdo5qeU0CAAC42eWyQU1A57Amst93mJEhnTsns9lPXl62lUjPnTvuxsKqtkWLFikoKEh9+vRxdykAAABAqXHPYU1UaNVSRUXJ17eu8vNPyWpNk9WaIYuF7mFJ7du3TydOnNBTTz2l559/Xl5e/M8KAAAAVQ+dw5oqLMz2MylJystz6h7m5hb/yAa4evTRRzVw4EA9/PDDGjlypLvLAQAAAMqEFkdNFR4unTwp5eZKBw5ILVpc1D1MpXtYCitWrHB3CQAAAMAVo3NYU3l5SVdfbfuZmSkdOiSzyZfuIQAAAFBDEQ5rMj8/qVkzyWSSUlKkI0fk62t7BIO9ewgAAACgZiAclkC1ftpHcLDUpInt95MnZU5OdXQPc3KOqKAgz43FobBqfS0CAADArQiHl+Dt7S2TyaTMzEx3l1KxwsKk+vVtvx85It/sAElmFRRkKitrl/Lz091aHi7IysqSZLs2AQAAgPLEgjSXYLFYFBoaquTkZOXm5iokJEReXl4ymUzuLq381aolZWXZHm1x6IjMjesp15Qswzin3Ny98vaOlLd3ner53asAwzCUlZWlkydPqlatWrJYLO4uCQAAANUM4fAy6tatK39/f508eVJpaWnuLqdiGYZtcZrsbOn0aRl1o5SvnPP3Hp6S2ewnb+8ImUwEE3epVauW6tat6+4yAAAAUA0RDi/DZDKpVq1aCg0NldVqVX5+vrtLqlhRUdI990i7dkmBgVLfvjrbu64OmN6SYWTJyytcTZtOV61a17u70hrH29ubjiEAAAAqDOGwhEwmk7y8vOTlVc3/yPz8pHfflW67Tfr5Z2nXLkW/IEVe/2cd6pOpY522ac+emxQd/TdFRg5RaGiszGbufwMAAACqumqedFAm9epJ27ZJa9dKb7whLV0qy8atar5RalwnQEd7Zykp7i0l1n9LFu8QhYXdovDw2xUW1ls+PnXcXT0AAACAMvCo1Uo3b96s2NhYBQQEKDo6WpMmTSp2GqdhGJoxY4aaNGkiPz8/dejQQatWrarkiqsxk0nq2VNaskQ6fFh6+mkpKkpeyVlq/JHU+V4p9g6T2vwjTaFPL1bKy/fpfx9FavsPnXXw4CSdODFXaWk/KC/vjLu/CQAAAFBhqlOG8ZjO4e7duxUXF6exY8fq7bff1u7duzVmzBhZrVa98MILLvs/99xzev311/Xmm2/qmmuu0TvvvKM77rhDP/74o9q2beuGb1CNNWggTZ0qxcdL//mP9Oab0vffyysrV7X+J9X634VdC7y3KLveFuXWkTIjpDN1pPyoQKlBQ5kbXi3vhtfIEnGVvIPqy9u7jnx8IuXtHSkvr1BWQgUAAECVUt0yjMnwkKdqDxs2TFlZWVq2bJlj7K233tJjjz2mkydPKigoyDF+9uxZ1a9fX5988okGDBjgGO/atauaN2+ujz76qETnPHr0qBo2bKgjR46oQYMG5fdlaoK8PGnPHmn7dtu9idu3y9i+TabUkq3oavWV8oOl/MDzP4NMKgjxlRHgJyPAVwoMkBEQKFNQoBQYLFNQsOQXKJN/oEz+QbaXX5BMASEy+4fI5Bsgs2+gTL6BMvkEyuwXKLN3gExmP5nN3jKZ7C8CKAAAAFyVJRu4I8NUJI/oHFqtVi1fvlzvvfee0/igQYP097//XRs3btQtt9ziGF+1apW8vLzUt29fl/2nT59eKTXXeN7eUps2tte990qSTIZhm4J64IB07Jh09KgKjhxWwR/7ZRw7InPiSVlOZUqSLLm2l+8p+wENSTnnX+XDMEmGl2RYpALL+Z9ekiySYTHJ8DLZflpMksUsw8skWUwyLGYZXmbJbJLMtve2382S2Wzb//zvMpku2mb7aZhNMplMMuzbTaaL9jVLJhV6f/4YJpNkuvDeMNmOYz+G03tdGHe8pELvi9h28U+z2bZPkdsu+pxtg/N2SSaz+fyfs+s2+/62ei/6fOH9Cod1k/kS2y4R7EtznLIes/Bm8yVm5V/is8Ylj1u2bZc+5BXcPXDJP4Iy/kOLJ/0DzWVr8aBaK0pF/B0DQCmZQmorZOBEd5dRatUxw3hEODx8+LAyMjJcWqlhYWGKiorS/v37nf5gf/31V8XExLgs6x8TE6MTJ04oIyPDKaXb5ebmKjc31/E+NTVVkpSYmFieX6dm8/aWWrWyvYpitUrp6VJamu2VmiprSrKMs4mypiTJyEiXkZUmIzNTysqQsrJkysqRcrJlys2Tcq0y5eXLdM4q07l8mfIKZMo1ZMqXzIV74IakvPMvF8b5FwAAANwpp76X0jrf4+4yHJkgNTVVISEhjnFfX1/5+vq67F9ZGaYyeUQ4TE5OliSFh4e7bAsLC3N5+HxycnKx+0pSWlpakX+wCQkJevbZZ13GO3XqVKa6AQAAAFyhY/lSw4bursKhdevWTu8nT56sKVOmuOxXWRmmMnlEOLSv5mMuYpqWyT6VrtD+xe178c/CJk6cqPHjxzsdZ/fu3WrYsGGRx6tM6enpiomJ0a5duxQcHOzWWlC1cO2gLLhuUBZcNygrrh2URWVfNwUFBfrjjz8UExPj9GzzorqGUuVlmMrkEeHQ3rZNTU11SdMpKSkuYyEhIdq3b5/LcVJSUmQymVS7du0iz1NUS/j666+/ktLLjf1fFurXr+/UxgYuh2sHZcF1g7LgukFZce2gLNxx3TRq1KjE+1ZWhqlMHvGcw2bNmslsNmvPnj1O46mpqUpMTHRp7bZo0cJlX0nas2ePmjdvLj8/vwqtFwAAAEDNVh0zjEeEw8DAQMXGxmrBggVO40uWLFFkZKQ6d+7sNH7zzTfr1KlTWrNmjdP4okWL1K9fvwqvFwAAAEDNVh0zjEdMK5Wkp59+WrfeeqtatWqlAQMGaMeOHXriiSf00ksvyWKxaMiQIercubPGjx+vpk2basSIERo9erTefPNNNWrUSG+//bb27t2rxYsXu/urlImvr68mT55c7JxmoDhcOygLrhuUBdcNyoprB2VRFa6b6pZhTIZheMx6/osWLdKUKVP022+/qXHjxnryySd1//33S7KtKBobG6uZM2dKkrKzs/Xkk09q/vz5ys7O1vXXX69Zs2YpJibGnV8BAAAAQA1SnTKMR4VDAAAAAIB7eMQ9hwAAAAAA9yIcAgAAAAAIhwAAAAAAwqFH2Lx5s2JjYxUQEKDo6GhNmjRJ+fn57i4LHiQpKUljxoxR3bp1FRAQoD/96U8uq1rt3btXt956q4KCghQREaG///3vyszMdFPF8ESjRo2SyWRSSkqKY4zrBpfy4Ycf6k9/+pP8/f1Vu3ZtTZgwwbGNawdFWbhwodq1ayd/f39dc801euWVV3Tx8hZcN7CzWq2aNWuWunTp4rKtJNfJ8ePHNWTIENWqVUshISEaOnSoTp48WVnlV1uEQzfbvXu34uLi1K1bN23dulWvvfaaZs+erfj4eHeXBg8yduxY5eTkaMmSJdq8ebNuu+02DR48WF9++aUk6eTJk+revbvq1aun77//XvPmzdOKFSscK2UB+/bt09y5c53GuG5wKZMnT9ZTTz2lRx55RNu3b9fXX3+tuLg4SVw7KNqKFSt09913a/jw4dq6daueeuopTZo0ybFKI9cNJNtqnXPmzFGbNm00YcIE5eTkOG0vyXWSm5uruLg4ZWZmavXq1VqxYoX27t2rfv36ibU2r5ABtxo6dKjRt29fp7E333zT8Pf3N9LT091UFTzNrl27XMbuuOMOY8CAAYZhGMb//d//Ge3btzesVqtj+8qVKw2TyWQcOHCg0uqE57rpppuMPn36GJKMs2fPGobBdYPibd++3fDz8zP27t1b5HauHRTlzjvvdPlvmqefftpo1aqVYRhcN7BZu3atERYWZkycONF44oknjHbt2jltL8l18uabbxp169Y1MjMzHfvs3r3bMJlMxurVqyvle1RXdA7dyGq1avny5Ro+fLjT+KBBg5STk6ONGze6qTJ4mlatWrmMtWzZ0jF9YunSpRo2bJjM5gv/k7755psVGhqqb775ptLqhGf66KOPlJiYqMcee8xpnOsGxXnllVd09913q0WLFkVu59pBUcxmswIDA53GgoKCZLVaJXHdwKZjx446fvy4pk2bpoCAAJftJblOli5dqgEDBjh9vmXLlmrTpo2+/vrriv8S1Rjh0I0OHz6sjIwMtW3b1mk8LCxMUVFR2r9/v5sqQ1Xw448/qnXr1jp37pz279/vch2ZTCa1bNmS66iGO3z4sB577DG99dZb8vLycoxz3eBSVq1apW7duumRRx5RdHS0IiMjdd999+nMmTNcOyjWP/7xDy1btkz/+c9/lJ+fr82bN2vmzJkaP3481w0cgoOD5evrW+S2kl4nv/76q8s+khQTE8O1dIUIh26UnJwsSQoPD3fZFhYWprS0tMouCVXE3Llz9cMPP+iRRx7RmTNnZLVauY7gIj8/X3fffbf+8Y9/qGvXrk7buG5QnPT0dB07dkyvvfaaLBaLli5dqtmzZ2v9+vUaOnQo1w6KdeONN2rKlCm688475ePjo7/85S+66aab9MADD3DdoERKep0kJydzLVUQwqEb2VckvbhtbmcymWQymSq7JHg4wzCUkJCghx56SPPmzVNMTAzXEYo1fvx4eXl56ZlnnnHZxnWD4tj/wyomJkYvv/yyOnfurIEDB2rBggX6+uuvHf8qz7WDwj766CO98MILev311/Xjjz9q3rx52rBhg5544gn+bw5KpKTXSX5+PtdSBfG6/C6oKCEhIZKk1NRUl3/9SElJKfJfRFBznTp1SiNGjNDBgwe1fv16dejQQZLzdVRYSkpKkdMuUP298847WrRokbZt2yaLxeKynesGxfH29pYk9enTx2m8c+fOCg4O1rZt2yRx7cBZenq6HnnkEb3zzjsaNGiQJNu9Ze3atVPr1q11zz33SOK6waWV9P83hYSEFLsP//18ZegculGzZs1kNpu1Z88ep/HU1FQlJiaqdevWbqoMnubEiRPq2rWrwsPDtX37dkcwlGz/BzIqKsrlOjIMQ3v37uU6qqGmTZumpKQk1a9f3/EvqT169JAk1a5dW4888gjXDYpUp04dBQUFFfkfXiaTif+bgyLt2rVLqampLlPYY2JiFBYWps2bN3Pd4LJK+n9fWrRo4bKPJO3Zs4dr6QoRDt0oMDBQsbGxWrBggdP4kiVLFBkZqc6dO7upMniav/3tb/rTn/6kTz75xGUlOEm65ZZbXK6jNWvWKCMjQ7fccktllQkP8uWXX2r79u1Or3feeUeStH79ek2dOpXrBkUymUyKi4vTwoULncY3btyo9PR0XX/99Vw7cFGvXj1JtsXSLrZ//36dPn1a0dHRXDcokZJcJ7fccos+/fRT5eXlOfbZv3+/fv75Z/Xt27dS66123PkcDRjG119/bVgsFmPatGnG7t27jUWLFhm1a9c23n//fXeXBg+RmZlpWCwW48MPPzQOHTrk8srPzzd27txp+Pn5GePGjTN27txp/Pe//zWuuuoq45lnnnF3+fAga9eudXrOIdcNirN161bDx8fH+Nvf/mb89NNPxrJly4xGjRoZo0aNMgyDawdFGzFihBEVFWV8+OGHxs6dO42lS5caLVu2NDp06GCcO3eO6wYuJk+e7PKcw5JcJ0lJSUZERIQxdOhQ4+effzbWr19vtG/f3rj33nsr+RtUP4RDD7Bw4UKjVatWho+Pj9GiRQvj3XffdXdJ8CC///67IanY15EjRwzDMIw1a9YYHTt2NHx8fIxGjRoZCQkJRkFBgZurhycpHA4Ng+sGxfv666+NP//5z4aPj48RHR1tPPXUU0ZeXp5jO9cOCjt37pzx4osvGjExMYa/v7/RtGlTY/z48UZKSopjH64bXKyocGgYJbtOfv75Z+OGG24w/Pz8jKioKGPChAlGTk5OJVVefZkMwzDc1rYEAAAAAHgE7jkEAAAAABAOAQAAAACEQwAAAACACIcAAAAAABEOAQAAAAAiHAIAAAAARDgEAAAAAIhwCAAAAAAQ4RAAgCvSvXt3Pfroo+4uAwCAK0Y4BAAAAAAQDgEAAAAAhEMAAAAAgAiHAIAqaMuWLbrxxhvl7++vunXrauLEibJarZKkkSNHqn///vrhhx/UtWtX+fv7q1GjRvrXv/7lcpxffvlF/fr1U+3ateXn56cOHTpowYIFLvv99ttvGjZsmOrUqSM/Pz+1bdtWe/fuddpn4cKFatWqlQIDA/WXv/xF27dvr5gvDwBABSEcAgCqlG3btql79+6KiYnRpk2b9MYbb+j999/XCy+84Njn0KFD+tvf/qbHH39cW7Zs0T/+8Q89/fTTevPNN52O07VrVwUEBGjZsmXauHGjbrvtNo0YMULvv/++Y78DBw6oU6dOSk5O1rx587RlyxaNGTNGOTk5jn2+/fZbvffee3rvvff01VdfKTc3V3fddZcjsAIAUBWYDMMw3F0EAAAl1atXL/n5+enLL790jM2dO1fjxo1TUlKSxowZo/nz5+vXX3/V1Vdf7djn//7v/zR37lwdO3bMcRxvb2+tXLnS6fgTJkzQJ598omPHjslsNqt///46fvy4Nm/eLLPZ9d9Uu3fvrn379unAgQMKCAiQJG3cuFGxsbH6+eef1a5du4r4YwAAoNzROQQAVBnZ2dlat26d7r//fqfxbt266ezZs/rjjz8kSX/605+cgqEk9evXT8ePH1dycrJycnK0fv16l+NI0vDhw3XixAnt3btX586d08qVKzVu3Lgig6Fdr169HMFQktq3by9JjiAKAEBV4OXuAgAAKKkzZ87IarVq2LBhMplMLtuPHz8uSYqKinLZFhoaKklKTk7WuXPnlJ+fr4YNG7rsFx0dLUk6e/asTp06pdzcXDVt2vSSdYWHhzu9DwwMlCSdO3euBN8KAADPQDgEAFQZoaGhMplMeuedd9SpUyeX7Y0aNdJ7772nrKwsl22///67JKlu3bry8fGRyWQqsrN34sQJSVKdOnUUFBQkSUpMTCzPrwEAgEdiWikAoMoICgpSu3bttHfvXrVs2dLlZZ/a+eOPP+r06dNOn507d646duyosLAwBQUFqUuXLnr33XddzjF//nw1b95cV199tUJCQtSxY0d98MEHlfH1AABwKzqHAIAqZfLkyRoyZIi8vb3Vt29fGYahH374QQcOHNDMmTMlST4+PurTp4+mTZum2rVr6+OPP9aiRYv03//+13Gc6dOnq1evXho2bJgeeughBQYGatmyZfr3v/+t//znP0773XLLLbrnnnv0wAMPKCgoSCtXrtRNN92kjh07Vvr3BwCgotA5BABUKf3799e8efO0dOlSde3aVXfccYdWrlypoUOHOvb5y1/+ojFjxmj06NHq2rWrNmzYoOXLl+umm25y7BMbG6u1a9cqOTlZvXv3Vrdu3bRu3Tp99dVX6t27t2O/nj17avXq1Tpy5Ihuvvlm9ejRQ999912R9zUCAFCV8SgLAEC1MnLkSKWkpGjp0qXuLgUAgCqFziEAAAAAgHAIAAAAACAcAgAAAADEPYcAAAAAANE5BAAAAACIcAgAAAAAEOEQAAAAACDCIQAAAABAhEMAAAAAgAiHAAAAAAARDgEAAAAAIhwCAAAAAEQ4BAAAAABI+v9/0frrZmTbAgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "# plt.figure(figsize=(6,4)) # ERROR\n",
    "fig.set_size_inches(10, 5)  # 챠트 크기 설정\n",
    "\n",
    "acc_ax = loss_ax.twinx()  # 오른쪽 y 출 설정\n",
    "\n",
    "# 왼쪽 y 축 설정\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5]) # 값을 반영하여 변경\n",
    "\n",
    "# 오른쪽 y 축 설정\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylim([0.0, 1.1])\n",
    "\n",
    "# 축 레이블 설정\n",
    "loss_ax.set_xlabel('epoch' )  # 학습 횟수\n",
    "loss_ax.set_ylabel('loss')    # 왼쪽 y 축 레이블, 오차\n",
    "acc_ax.set_ylabel('accuracy') # 오른쪽 y 축 레이블,정확도\n",
    "\n",
    "loss_ax.legend(loc='upper left') # 왼쪽 y 축 오차 레이블 위치\n",
    "acc_ax.legend(loc='lower left')  # 오른쪽 y 축 정확도 레이블 위치\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "638e83e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손실값: 2.533196834519913e-07 /정확도: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_val, y_val, batch_size=1, verbose=0)\n",
    "print('손실값:', test_loss, '/정확도:', (test_acc*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(path + '/Pinterest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5419678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: (22, 49)\n",
      "데이터: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0.]\n",
      "예측 결과 p.shape: (22, 7)\n"
     ]
    }
   ],
   "source": [
    "print('데이터:', x_test.shape) # 변수가 25개로 구성된 10건의 관측치(행)\n",
    "print('데이터:', x_test[0])    # 첫번째 데이터행\n",
    "\n",
    "p = model.predict(x_test)      # 테스트 데이터 10건 ★\n",
    "print('예측 결과 p.shape:', p.shape)     # (10, 5): 5: 카테고리(폼종)의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668d3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [1.1664235e-07 9.9999988e-01 2.4563944e-08 6.0457275e-09 1.3402797e-09\n",
      " 7.7899660e-09 3.1085694e-08]\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 행의 예측값 출력, 확률 0 ~ 1사이의 실수값\n",
    "print('예측값:', p[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a9a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값의 합: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 예측값의 합:1 -> 100%\n",
    "print('예측값의 합: {0:0.3f}'.format(np.sum(p[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f752661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: 0.00001% 99.99999% 0.00000% 0.00000% 0.00000\n"
     ]
    }
   ],
   "source": [
    "# 하나의 행이 속할 카테고리는 5건이고 예측값도 5건에 속할 확률이 출력됨.\n",
    "print('예측값: {0:.5f}% {1:.5f}% {2:.5f}% {3:.5f}% {4:.5f}'.format(p[0,0]*100,p[0,1]*100,p[0,2]*100,p[0,3]*100,p[0,4]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86417ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding(찾아야할 값):  [0. 1. 0. 0. 0. 0. 0.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('One-hot-encoding(찾아야할 값): ', y_test[0])\n",
    "print(np.argmax(p[0])) # 가장 큰값의 index ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb3571bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.16642347e-07 9.99999881e-01 2.45639438e-08 6.04572747e-09\n",
      "  1.34027967e-09 7.78996601e-09 3.10856940e-08]\n",
      " [8.04437003e-11 1.84208595e-08 9.99999881e-01 1.92115344e-18\n",
      "  9.06723798e-08 1.35054234e-08 1.29214098e-10]\n",
      " [1.16642347e-07 9.99999881e-01 2.45639438e-08 6.04572747e-09\n",
      "  1.34027967e-09 7.78996601e-09 3.10856940e-08]\n",
      " [1.00000000e+00 1.12140439e-08 8.07429557e-12 5.28085753e-09\n",
      "  1.91176177e-08 1.87981950e-08 4.94137581e-17]\n",
      " [7.74339313e-08 2.61323585e-09 1.16867160e-12 9.99999642e-01\n",
      "  2.21267825e-07 7.96548605e-09 2.75356093e-08]\n",
      " [7.59031781e-13 5.83864761e-08 1.31537575e-07 6.49104663e-08\n",
      "  4.41026984e-08 1.55433250e-10 9.99999762e-01]\n",
      " [8.04437003e-11 1.84208595e-08 9.99999881e-01 1.92115344e-18\n",
      "  9.06723798e-08 1.35054234e-08 1.29214098e-10]\n",
      " [8.04437003e-11 1.84208595e-08 9.99999881e-01 1.92115344e-18\n",
      "  9.06723798e-08 1.35054234e-08 1.29214098e-10]\n",
      " [4.21581241e-08 1.30150994e-07 4.54107649e-08 3.13201163e-08\n",
      "  7.76024578e-09 9.99999762e-01 1.42780685e-08]\n",
      " [7.74339313e-08 2.61323585e-09 1.16867160e-12 9.99999642e-01\n",
      "  2.21267825e-07 7.96548605e-09 2.75356093e-08]\n",
      " [1.00000000e+00 1.12140439e-08 8.07429557e-12 5.28085753e-09\n",
      "  1.91176177e-08 1.87981950e-08 4.94137581e-17]\n",
      " [4.21581241e-08 1.30150994e-07 4.54107649e-08 3.13201163e-08\n",
      "  7.76024578e-09 9.99999762e-01 1.42780685e-08]\n",
      " [1.10065823e-08 7.64529273e-10 1.97568028e-07 2.32550377e-07\n",
      "  9.99999523e-01 2.17650964e-08 2.69101852e-10]\n",
      " [4.21581241e-08 1.30150994e-07 4.54107649e-08 3.13201163e-08\n",
      "  7.76024578e-09 9.99999762e-01 1.42780685e-08]\n",
      " [7.59031781e-13 5.83864761e-08 1.31537575e-07 6.49104663e-08\n",
      "  4.41026984e-08 1.55433250e-10 9.99999762e-01]\n",
      " [1.10065823e-08 7.64529273e-10 1.97568028e-07 2.32550377e-07\n",
      "  9.99999523e-01 2.17650964e-08 2.69101852e-10]\n",
      " [1.16642347e-07 9.99999881e-01 2.45639438e-08 6.04572747e-09\n",
      "  1.34027967e-09 7.78996601e-09 3.10856940e-08]\n",
      " [7.74339313e-08 2.61323585e-09 1.16867160e-12 9.99999642e-01\n",
      "  2.21267825e-07 7.96548605e-09 2.75356093e-08]\n",
      " [1.10065823e-08 7.64529273e-10 1.97568028e-07 2.32550377e-07\n",
      "  9.99999523e-01 2.17650964e-08 2.69101852e-10]\n",
      " [1.00000000e+00 1.12140439e-08 8.07429557e-12 5.28085753e-09\n",
      "  1.91176177e-08 1.87981950e-08 4.94137581e-17]\n",
      " [1.00000000e+00 1.12140439e-08 8.07429557e-12 5.28085753e-09\n",
      "  1.91176177e-08 1.87981950e-08 4.94137581e-17]\n",
      " [7.59031781e-13 5.83864761e-08 1.31537575e-07 6.49104663e-08\n",
      "  4.41026984e-08 1.55433250e-10 9.99999762e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(p) # 10건의 모든 예측값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5ae37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
